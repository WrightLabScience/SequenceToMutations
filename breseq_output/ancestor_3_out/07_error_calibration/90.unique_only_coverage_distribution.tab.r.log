
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ##
> ##
> ## AUTHORS
> ##
> ## Jeffrey E. Barrick <jeffrey.e.barrick@gmail.com>
> ##
> ## LICENSE AND COPYRIGHT
> ##
> ## Copyright (c) 2008-2010 Michigan State University
> ## Copyright (c) 2011-2022 The University of Texas at Austin
> ##
> ## breseq is free software; you can redistribute it and/or modify it under the
> ## terms the GNU General Public License as published by the Free Software
> ## Foundation; either version 1, or (at your option) any later version.
> ##
> ##
> 
> ## Arguments:
> ##   distribution_file=/path/to/input 
> ##   plot_file=/path/to/output 
> ##   deletion_propagation_pr_cutoff=float
> ##   plot_poisson=0 or 1
> ##   pdf_output=0 or 1
> 
> ## Returns these values printed out to output log
> ## 
> ##  1. print(nb_fit_size); # 0 if fit failed
> ##  2. print(nb_fit_mu);   # 0 if fit failed
> ##  3. print(m)q
> ##  4. print(v)
> ##  5. print(D)
> ##  6. print(deletion_propagation_coverage)
> ##     -1 if it was <1 after fitting (implying reference sequence is deleted)
> ##
> 
> plot_poisson = 0;
> pdf_output = 1;
> 
> this.print.level = 0
> #this.print.level = 2
> 
> for (e in commandArgs(TRUE)) {
+   ta = strsplit(e,"=",fixed=TRUE)[[1]]
+   if(length(ta)>1) {
+     temp = ta[2]
+     assign(ta[1],temp)
+     cat("assigned ",ta[1]," the value of |",temp,"|\n")
+   } else {
+     assign(ta[[1]][1],TRUE)
+     cat("assigned ",ta[1]," the value of TRUE\n")
+   }
+ }
assigned  distribution_file  the value of | p3_out/07_error_calibration/90.unique_only_coverage_distribution.tab |
assigned  plot_file  the value of | p3_out/output/calibration/90.unique_coverage.pdf |
assigned  deletion_propagation_pr_cutoff  the value of | 0.00278207 |
> 
> deletion_propagation_pr_cutoff = as.numeric(deletion_propagation_pr_cutoff);
> 
> ## initialize values to be filled in
> nb_fit_mu = 0
> nb_fit_size = 0
> m = 0
> v = 0
> D = 0
> deletion_propagation_coverage = -1
> 
> min_fraction_included_in_nb_fit = 0.01
> 
> #load data
> X<-read.table(distribution_file, header=T)
> 
> #table might be empty
> if (nrow(X) == 0)
+ {
+   #print out statistics
+   
+   print(nb_fit_size);
+   print(nb_fit_mu);
+   
+   print(m)
+   print(v)
+   print(D)
+   
+   print(deletion_propagation_coverage)
+   
+   q()
+ }
> 
> #create the distribution vector and fit
> Y<-rep(X$coverage, X$n)
> m<-mean(Y)
> v<-var(Y)
> D<-v/m
> 
> ###
> ## Smooth the distribution with a moving average window of size 5
> ## so that we can more reliably find it's maximum value
> ###
> 
> ma5 = c(1, 1, 1, 1, 1)/5;
> 
> ## filtering fails if there are too few points
> if (nrow(X) >= 5) {
+   X$ma = filter(X$n, ma5)
+ } else {
+ 	X$ma = X$n
+ }
> 
> i<-0
> max_n <- 0;
> min_i <- max( trunc(m/4), 1 ); #prevents zero for pathological distributions
> max_i <- i;
> for (i in min_i:length(X$ma))
+ {		
+   #cat(i, "\n")
+ 	if (!is.na(X$ma[i]) && (X$ma[i] > max_n))
+ 	{
+ 		max_n = X$ma[i];
+ 		max_i = i;
+ 	}
+ }
> 
> ##
> # Censor data on the right and left of the maximum
> ##
> 
> start_i = max(floor(max_i*0.5), 1);
> end_i = min(ceiling(max_i*1.5), length(X$ma));
> 
> if (start_i == end_i)
+ {
+   print(nb_fit_size);
+   print(nb_fit_mu);
+   
+   print(m)
+   print(v)
+   print(D)
+   
+   print(deletion_propagation_coverage)
+   
+   q()
+ }
> 
> cat("Fitting from coverage of ", start_i, " to ", end_i, ".\n", sep="")
Fitting from coverage of 41 to 89.
> 
> ##
> # Coarse grain so that we are only fitting a number of bins that is 1000-2000
> #
> # The later adjustment for doing the fits this way is to multiply the means
> # of the negative binomial and poisson distributions by the binning number.
> # (The size parameter of the negative binomial doesn't need to be adjusted.)
> ##
> 
> 
> num_per_bin = trunc((end_i - start_i) / 1000)
> 
> if (num_per_bin > 1) 
+ {
+   cat("Coarse-graining for fits\n")
+   start_i_for_fits = trunc(start_i/num_per_bin)
+   end_i_for_fits = ceiling(end_i/num_per_bin)
+   num_bins = end_i - start_i  + 1
+   cat("Fitting from coverage in adjusted bins ", start_i_for_fits, " to ", end_i_for_fits, ".\n", sep="")
+   cat("Number of bins ", num_bins, ". Each bin has ", num_per_bin, " coverage values.\n", sep="")
+ 
+   # Create a new vector where we've added together values in bins
+   X.for.fits = vector("double", end_i_for_fits)
+   for (i in start_i_for_fits:end_i_for_fits)
+   {
+     for (j in 1:num_per_bin)
+     {
+       if (i*num_per_bin+j <= length(X$n))
+       {
+         X.for.fits[i] = X.for.fits[i] + X$n[i*num_per_bin+j]
+       }
+     }
+   }
+ 
+ } else {
+   ## AVOID num_per_bin equalling zero!!
+   X.for.fits = X$n[1:end_i]
+   num_per_bin = 1
+   start_i_for_fits = start_i
+   end_i_for_fits = end_i
+ }
> 
> 
> ##
> # Now perform negative binomial fitting to the censored data
> ##
> 
> inner_total<-0;
> for (i in start_i_for_fits:end_i_for_fits)
+ {
+ 	inner_total = inner_total + X.for.fits[i]; 
+ }
> # Yes: it's correct to use X here because we want the overall total total
> total_total<-sum(X$n);
> 
> ## let's preconstruct these for speed
> dist = vector("double", end_i_for_fits)
> 
> f_nb <- function(par) {
+ 
+ 	mu = par[1];
+ 	size = par[2];
+ 
+   if ((mu <= 0) || (size <= 0))
+   {
+     return(0);
+   }
+   
+   cat(start_i_for_fits, " ", end_i_for_fits, "\n");
+   cat(mu, " ", size, "\n");
+   
+ 	dist<-c()
+ 	total <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{	
+ 		dist[i] <- dnbinom(i, size=size, mu=mu);
+ 		total <- total + dist[i] 
+ 	}
+ 	#print (mu, size)
+ 
+  	l <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{
+ 		l <- l + ((X.for.fits[i]/inner_total)-(dist[i]/total))^2;
+ 	}
+ 	return(l);
+ }
> 
> 
> 
> ## Fit negative binomial 
> ## - allow fit to fail and set all params to zero/empty if that is the case
> nb_fit = NULL
> ## as.numeric prevents overflow in sums involving integers
> mean_estimate = sum((as.numeric(1:end_i_for_fits)*as.numeric(X.for.fits)))/sum(as.numeric(X.for.fits))
> 
> nb_fit_mu = -1
> nb_fit_size = -1
> try_size = 100000
> try_means_index = 1
> #This is a list of different means to test <-  sometimes the actual mean doesn't lead to a fit
> try_means = c(mean_estimate, 
+               end_i_for_fits, 
+               start_i_for_fits, 
+               1*(end_i_for_fits + start_i_for_fits)/4,
+               2*(end_i_for_fits + start_i_for_fits)/4,
+               3*(end_i_for_fits + start_i_for_fits)/4
+               )
>               
>               
> nb_fit = c()
> 
> while ( ((nb_fit_mu < 0) || (nb_fit_size < 0) || (nb_fit$code != 1)) && (try_size > 0.001) && (try_means_index <= length(try_means)))
+ {
+   try_size = try_size / 10
+   try_mean = try_means[try_means_index]
+ 
+   ## SIZE ESTIMATE from the censored data can be negative, so try various values instead
+   cat("Try Mean: ", try_mean, " Size: ", try_size, "\n")
+ 
+   try( suppressWarnings(nb_fit<-nlm(f_nb, c(try_mean, try_size), iterlim=1000, print.level=this.print.level)) )
+ 
+   nb_fit_mu = nb_fit$estimate[1];
+   nb_fit_size = nb_fit$estimate[2];
+ 
+   cat("Fit Mean: ", nb_fit_mu, " Size: ", nb_fit_size, " Code: ", nb_fit$code, "\n")
+   
+   if (try_size <= 0.001) {
+     try_size = 100000
+     try_means_index = try_means_index + 1
+   }
+ }
Try Mean:  66.6055  Size:  10000 
41   89 
66.6055   10000 
41   89 
66.6055   10000 
41   89 
66.60557   10000 
41   89 
66.6055   10000.01 
41   89 
66.6079   10000 
41   89 
66.60796   10000 
41   89 
66.6079   10000.01 
41   89 
66.61029   10000 
41   89 
66.61035   10000 
41   89 
66.61029   10000.01 
41   89 
66.61268   10000 
41   89 
66.61274   10000 
41   89 
66.61268   10000.01 
41   89 
66.61507   10000 
41   89 
66.61513   10000 
41   89 
66.61507   10000.01 
41   89 
66.61746   10000 
41   89 
66.61752   10000 
41   89 
66.61746   10000.01 
41   89 
66.61985   10000 
41   89 
66.61991   10000 
41   89 
66.61985   10000.01 
41   89 
66.62224   10000 
41   89 
66.6223   10000 
41   89 
66.62224   10000.01 
41   89 
66.62463   10000 
41   89 
66.6247   10000 
41   89 
66.62463   10000.01 
41   89 
66.62702   10000 
41   89 
66.62709   10000 
41   89 
66.62702   10000.01 
41   89 
66.62941   10000 
41   89 
66.62948   10000 
41   89 
66.62941   10000.01 
41   89 
66.6318   10000 
41   89 
66.63187   10000 
41   89 
66.6318   10000.01 
41   89 
66.63419   10000 
41   89 
66.63426   10000 
41   89 
66.63419   10000.01 
41   89 
66.63658   10000 
41   89 
66.63665   10000 
41   89 
66.63658   10000.01 
41   89 
66.63897   10000 
41   89 
66.63904   10000 
41   89 
66.63897   10000.01 
41   89 
66.64136   10000 
41   89 
66.64143   10000 
41   89 
66.64136   10000.01 
41   89 
66.64375   10000 
41   89 
66.64382   10000 
41   89 
66.64375   10000.01 
41   89 
66.64615   10000 
41   89 
66.64621   10000 
41   89 
66.64615   10000.01 
41   89 
66.64854   10000 
41   89 
66.6486   10000 
41   89 
66.64854   10000.01 
41   89 
66.65093   10000 
41   89 
66.65099   10000 
41   89 
66.65093   10000.01 
41   89 
66.65332   10000 
41   89 
66.65339   10000 
41   89 
66.65332   10000.01 
41   89 
66.65571   10000 
41   89 
66.65578   10000 
41   89 
66.65571   10000.01 
41   89 
66.6581   10000 
41   89 
66.65817   10000 
41   89 
66.6581   10000.01 
41   89 
66.66049   10000 
41   89 
66.66056   10000 
41   89 
66.66049   10000.01 
41   89 
66.66288   10000 
41   89 
66.66295   10000 
41   89 
66.66288   10000.01 
41   89 
66.66527   10000 
41   89 
66.66534   10000 
41   89 
66.66527   10000.01 
41   89 
66.66766   10000 
41   89 
66.66773   10000 
41   89 
66.66766   10000.01 
41   89 
66.67006   10000 
41   89 
66.67012   10000 
41   89 
66.67006   10000.01 
41   89 
66.67245   10000 
41   89 
66.67251   10000 
41   89 
66.67245   10000.01 
41   89 
66.67484   10000 
41   89 
66.67491   10000 
41   89 
66.67484   10000.01 
41   89 
66.67723   10000 
41   89 
66.6773   10000 
41   89 
66.67723   10000.01 
41   89 
66.67962   10000 
41   89 
66.67969   10000 
41   89 
66.67962   10000.01 
41   89 
66.68201   10000 
41   89 
66.68208   10000 
41   89 
66.68201   10000.01 
41   89 
66.6844   10000 
41   89 
66.68447   10000 
41   89 
66.6844   10000.01 
41   89 
66.68679   10000 
41   89 
66.68686   10000 
41   89 
66.68679   10000.01 
41   89 
66.68919   10000 
41   89 
66.68925   10000 
41   89 
66.68919   10000.01 
41   89 
66.69158   10000 
41   89 
66.69164   10000 
41   89 
66.69158   10000.01 
41   89 
66.69397   10000 
41   89 
66.69404   10000 
41   89 
66.69397   10000.01 
41   89 
66.69636   10000 
41   89 
66.69643   10000 
41   89 
66.69636   10000.01 
41   89 
66.69875   10000 
41   89 
66.69882   10000 
41   89 
66.69875   10000.01 
41   89 
66.70114   10000 
41   89 
66.70121   10000 
41   89 
66.70114   10000.01 
41   89 
66.70353   10000 
41   89 
66.7036   10000 
41   89 
66.70353   10000.01 
41   89 
66.70593   10000 
41   89 
66.70599   10000 
41   89 
66.70593   10000.01 
41   89 
66.70832   10000 
41   89 
66.70838   10000 
41   89 
66.70832   10000.01 
41   89 
66.71071   10000 
41   89 
66.71078   10000 
41   89 
66.71071   10000.01 
41   89 
66.7131   10000 
41   89 
66.71317   10000 
41   89 
66.7131   10000.01 
41   89 
66.71549   10000 
41   89 
66.71556   10000 
41   89 
66.71549   10000.01 
41   89 
66.71788   10000 
41   89 
66.71795   10000 
41   89 
66.71788   10000.01 
41   89 
66.72028   10000 
41   89 
66.72034   10000 
41   89 
66.72028   10000.01 
41   89 
66.72267   10000 
41   89 
66.72273   10000 
41   89 
66.72267   10000.01 
41   89 
66.72506   10000 
41   89 
66.72513   10000 
41   89 
66.72506   10000.01 
41   89 
66.72745   10000 
41   89 
66.72752   10000 
41   89 
66.72745   10000.01 
41   89 
66.72984   10000 
41   89 
66.72991   10000 
41   89 
66.72984   10000.01 
41   89 
66.73223   10000 
41   89 
66.7323   10000 
41   89 
66.73223   10000.01 
41   89 
66.73463   10000 
41   89 
66.73469   10000 
41   89 
66.73463   10000.01 
41   89 
66.73702   10000 
41   89 
66.73708   10000 
41   89 
66.73702   10000.01 
41   89 
66.73941   10000 
41   89 
66.73948   10000 
41   89 
66.73941   10000.01 
41   89 
66.7418   10000 
41   89 
66.74187   10000 
41   89 
66.7418   10000.01 
41   89 
66.74419   10000 
41   89 
66.74426   10000 
41   89 
66.74419   10000.01 
41   89 
66.74658   10000 
41   89 
66.74665   10000 
41   89 
66.74658   10000.01 
41   89 
66.74898   10000 
41   89 
66.74904   10000 
41   89 
66.74898   10000.01 
41   89 
66.75137   10000 
41   89 
66.75143   10000 
41   89 
66.75137   10000.01 
41   89 
66.75376   10000 
41   89 
66.75383   10000 
41   89 
66.75376   10000.01 
41   89 
66.75615   10000 
41   89 
66.75622   10000 
41   89 
66.75615   10000.01 
41   89 
66.75854   10000 
41   89 
66.75861   10000 
41   89 
66.75854   10000.01 
41   89 
66.76094   10000 
41   89 
66.761   10000 
41   89 
66.76094   10000.01 
41   89 
66.76333   10000 
41   89 
66.76339   10000 
41   89 
66.76333   10000.01 
41   89 
66.76572   10000 
41   89 
66.76579   10000 
41   89 
66.76572   10000.01 
41   89 
66.76811   10000 
41   89 
66.76818   10000 
41   89 
66.76811   10000.01 
41   89 
66.7705   10000 
41   89 
66.77057   10000 
41   89 
66.7705   10000.01 
41   89 
66.7729   10000 
41   89 
66.77296   10000 
41   89 
66.7729   10000.01 
41   89 
66.77529   10000 
41   89 
66.77535   10000 
41   89 
66.77529   10000.01 
41   89 
66.77768   10000 
41   89 
66.77775   10000 
41   89 
66.77768   10000.01 
41   89 
66.78007   10000 
41   89 
66.78014   10000 
41   89 
66.78007   10000.01 
41   89 
66.78246   10000 
41   89 
66.78253   10000 
41   89 
66.78246   10000.01 
41   89 
66.78486   10000 
41   89 
66.78492   10000 
41   89 
66.78486   10000.01 
41   89 
66.78725   10000 
41   89 
66.78731   10000 
41   89 
66.78725   10000.01 
41   89 
66.78964   10000 
41   89 
66.78971   10000 
41   89 
66.78964   10000.01 
41   89 
66.79203   10000 
41   89 
66.7921   10000 
41   89 
66.79203   10000.01 
41   89 
66.79442   10000 
41   89 
66.79449   10000 
41   89 
66.79442   10000.01 
41   89 
66.79682   10000 
41   89 
66.79688   10000 
41   89 
66.79682   10000.01 
41   89 
66.79921   10000 
41   89 
66.79928   10000 
41   89 
66.79921   10000.01 
41   89 
66.8016   10000 
41   89 
66.80167   10000 
41   89 
66.8016   10000.01 
41   89 
66.80399   10000 
41   89 
66.80406   10000 
41   89 
66.80399   10000.01 
41   89 
66.80639   10000 
41   89 
66.80645   10000 
41   89 
66.80639   10000.01 
41   89 
66.80878   10000 
41   89 
66.80884   10000 
41   89 
66.80878   10000.01 
41   89 
66.81117   10000 
41   89 
66.81124   10000 
41   89 
66.81117   10000.01 
41   89 
66.81356   10000 
41   89 
66.81363   10000 
41   89 
66.81356   10000.01 
41   89 
66.81595   10000 
41   89 
66.81602   10000 
41   89 
66.81595   10000.01 
41   89 
66.81835   10000 
41   89 
66.81841   10000 
41   89 
66.81835   10000.01 
41   89 
66.82074   10000 
41   89 
66.82081   10000 
41   89 
66.82074   10000.01 
41   89 
66.82313   10000 
41   89 
66.8232   10000 
41   89 
66.82313   10000.01 
41   89 
66.82552   10000 
41   89 
66.82559   10000 
41   89 
66.82552   10000.01 
41   89 
66.82792   10000 
41   89 
66.82798   10000 
41   89 
66.82792   10000.01 
41   89 
66.83031   10000 
41   89 
66.83038   10000 
41   89 
66.83031   10000.01 
41   89 
66.8327   10000 
41   89 
66.83277   10000 
41   89 
66.8327   10000.01 
41   89 
66.83509   10000 
41   89 
66.83516   10000 
41   89 
66.83509   10000.01 
41   89 
66.83749   10000 
41   89 
66.83755   10000 
41   89 
66.83749   10000.01 
41   89 
66.83988   10000 
41   89 
66.83994   10000 
41   89 
66.83988   10000.01 
41   89 
66.84227   10000 
41   89 
66.84234   10000 
41   89 
66.84227   10000.01 
41   89 
66.84466   10000 
41   89 
66.84473   10000 
41   89 
66.84466   10000.01 
41   89 
66.84706   10000 
41   89 
66.84712   10000 
41   89 
66.84706   10000.01 
41   89 
66.84945   10000 
41   89 
66.84951   10000 
41   89 
66.84945   10000.01 
41   89 
66.85184   10000 
41   89 
66.85191   10000 
41   89 
66.85184   10000.01 
41   89 
66.85423   10000 
41   89 
66.8543   10000 
41   89 
66.85423   10000.01 
41   89 
66.85663   10000 
41   89 
66.85669   10000 
41   89 
66.85663   10000.01 
41   89 
66.85902   10000 
41   89 
66.85908   10000 
41   89 
66.85902   10000.01 
41   89 
66.86141   10000 
41   89 
66.86148   10000 
41   89 
66.86141   10000.01 
41   89 
66.8638   10000 
41   89 
66.86387   10000 
41   89 
66.8638   10000.01 
41   89 
66.8662   10000 
41   89 
66.86626   10000 
41   89 
66.8662   10000.01 
41   89 
66.86859   10000 
41   89 
66.86865   10000 
41   89 
66.86859   10000.01 
41   89 
66.87098   10000 
41   89 
66.87105   10000 
41   89 
66.87098   10000.01 
41   89 
66.87337   10000 
41   89 
66.87344   10000 
41   89 
66.87337   10000.01 
41   89 
66.87577   10000 
41   89 
66.87583   10000 
41   89 
66.87577   10000.01 
41   89 
66.87816   10000 
41   89 
66.87822   10000 
41   89 
66.87816   10000.01 
41   89 
66.88055   10000 
41   89 
66.88062   10000 
41   89 
66.88055   10000.01 
41   89 
66.88294   10000 
41   89 
66.88301   10000 
41   89 
66.88294   10000.01 
41   89 
66.88534   10000 
41   89 
66.8854   10000 
41   89 
66.88534   10000.01 
41   89 
66.88773   10000 
41   89 
66.88779   10000 
41   89 
66.88773   10000.01 
41   89 
66.89012   10000 
41   89 
66.89019   10000 
41   89 
66.89012   10000.01 
41   89 
66.89251   10000 
41   89 
66.89258   10000 
41   89 
66.89251   10000.01 
41   89 
66.89491   10000 
41   89 
66.89497   10000 
41   89 
66.89491   10000.01 
41   89 
66.8973   10000 
41   89 
66.89737   10000 
41   89 
66.8973   10000.01 
41   89 
66.89969   10000 
41   89 
66.89976   10000 
41   89 
66.89969   10000.01 
41   89 
66.90208   10000 
41   89 
66.90215   10000 
41   89 
66.90208   10000.01 
41   89 
66.90448   10000 
41   89 
66.90454   10000 
41   89 
66.90448   10000.01 
41   89 
66.90687   10000 
41   89 
66.90694   10000 
41   89 
66.90687   10000.01 
41   89 
66.90926   10000 
41   89 
66.90933   10000 
41   89 
66.90926   10000.01 
41   89 
66.91165   10000 
41   89 
66.91172   10000 
41   89 
66.91165   10000.01 
41   89 
66.91405   10000 
41   89 
66.91411   10000 
41   89 
66.91405   10000.01 
41   89 
66.91644   10000 
41   89 
66.91651   10000 
41   89 
66.91644   10000.01 
41   89 
66.91883   10000 
41   89 
66.9189   10000 
41   89 
66.91883   10000.01 
41   89 
66.92122   10000 
41   89 
66.92129   10000 
41   89 
66.92122   10000.01 
41   89 
66.92362   10000 
41   89 
66.92368   10000 
41   89 
66.92362   10000.01 
41   89 
66.92601   10000 
41   89 
66.92608   10000 
41   89 
66.92601   10000.01 
41   89 
66.9284   10000 
41   89 
66.92847   10000 
41   89 
66.9284   10000.01 
41   89 
66.93079   10000 
41   89 
66.93086   10000 
41   89 
66.93079   10000.01 
41   89 
66.93319   10000 
41   89 
66.93325   10000 
41   89 
66.93319   10000.01 
41   89 
66.93558   10000 
41   89 
66.93565   10000 
41   89 
66.93558   10000.01 
41   89 
66.93797   10000 
41   89 
66.93804   10000 
41   89 
66.93797   10000.01 
41   89 
66.94037   10000 
41   89 
66.94043   10000 
41   89 
66.94037   10000.01 
41   89 
66.94276   10000 
41   89 
66.94282   10000 
41   89 
66.94276   10000.01 
41   89 
66.94515   10000 
41   89 
66.94522   10000 
41   89 
66.94515   10000.01 
41   89 
66.94754   10000 
41   89 
66.94761   10000 
41   89 
66.94754   10000.01 
41   89 
66.94994   10000 
41   89 
66.95   10000 
41   89 
66.94994   10000.01 
41   89 
66.95233   10000 
41   89 
66.9524   10000 
41   89 
66.95233   10000.01 
41   89 
66.95472   10000 
41   89 
66.95479   10000 
41   89 
66.95472   10000.01 
41   89 
66.95711   10000 
41   89 
66.95718   10000 
41   89 
66.95711   10000.01 
41   89 
66.95951   10000 
41   89 
66.95957   10000 
41   89 
66.95951   10000.01 
41   89 
66.9619   10000 
41   89 
66.96197   10000 
41   89 
66.9619   10000.01 
41   89 
33996.5   9999.839 
41   89 
3459.915   9999.984 
41   89 
406.2572   9999.998 
41   89 
235.3461   9999.999 
41   89 
96.04397   10000 
41   89 
77.48666   10000 
41   89 
77.48673   10000 
41   89 
77.48666   10000.01 
41   89 
79.66828   10000 
41   89 
79.66835   10000 
41   89 
79.66828   10000.01 
41   89 
78.77115   10000 
41   89 
78.77123   10000 
41   89 
78.77115   10000.01 
41   89 
78.77323   10000 
41   89 
78.77331   10000 
41   89 
78.77323   10000.01 
41   89 
78.77323   10000 
41   89 
78.78111   10000 
41   89 
78.76535   10000 
41   89 
78.77323   10001 
41   89 
78.77323   9999 
41   89 
78.77327   10000 
41   89 
78.78114   10000 
41   89 
78.76539   10000 
41   89 
78.77327   10001 
41   89 
78.77327   9999 
Fit Mean:  78.77327  Size:  10000  Code:  2 
Try Mean:  66.6055  Size:  1000 
41   89 
66.6055   1000 
41   89 
66.6055   1000 
41   89 
66.60557   1000 
41   89 
66.6055   1000.001 
41   89 
66.60783   1000 
41   89 
66.6079   1000 
41   89 
66.60783   1000.001 
41   89 
2568.647   1183.443 
41   89 
316.8117   1018.344 
41   89 
182.3958   1008.489 
41   89 
87.42058   1001.526 
41   89 
87.42067   1001.526 
41   89 
87.42058   1001.527 
41   89 
77.32887   999.4388 
41   89 
77.32894   999.4388 
41   89 
77.32887   999.4398 
41   89 
79.05338   999.3258 
41   89 
79.05346   999.3258 
41   89 
79.05338   999.3268 
41   89 
78.8594   999.0753 
41   89 
78.85948   999.0753 
41   89 
78.8594   999.0763 
41   89 
78.85868   998.8137 
41   89 
78.85876   998.8137 
41   89 
78.85868   998.8147 
41   89 
78.44824   717.1965 
41   89 
78.44832   717.1965 
41   89 
78.44824   717.1972 
Fit Mean:  36.03669  Size:  -30547.2  Code:  1 
Try Mean:  66.6055  Size:  100 
41   89 
66.6055   100 
41   89 
66.6055   100 
41   89 
66.60557   100 
41   89 
66.6055   100.0001 
41   89 
66.60732   99.99996 
41   89 
66.60738   99.99996 
41   89 
66.60732   100.0001 
41   89 
92.54262   100.5372 
41   89 
92.54271   100.5372 
41   89 
92.54262   100.5373 
41   89 
81.74846   99.30512 
41   89 
81.74854   99.30512 
41   89 
81.74846   99.30522 
41   89 
79.47091   99.13189 
41   89 
79.47099   99.13189 
41   89 
79.47091   99.13199 
41   89 
80.16984   99.09136 
41   89 
80.16992   99.09136 
41   89 
80.16984   99.09146 
41   89 
80.15699   99.0056 
41   89 
80.15707   99.0056 
41   89 
80.15699   99.0057 
41   89 
79.95728   95.8928 
41   89 
79.95736   95.8928 
41   89 
79.95728   95.8929 
41   89 
79.79054   89.08616 
41   89 
79.79062   89.08616 
41   89 
79.79054   89.08625 
41   89 
79.63661   67.00024 
41   89 
79.63669   67.00024 
41   89 
79.63661   67.0003 
41   89 
79.9187   57.23838 
41   89 
79.91878   57.23838 
41   89 
79.9187   57.23844 
41   89 
81.58909   42.23876 
41   89 
81.58917   42.23876 
41   89 
81.58909   42.2388 
41   89 
83.28443   47.5896 
41   89 
83.28451   47.5896 
41   89 
83.28443   47.58965 
41   89 
83.60523   35.71961 
41   89 
83.60532   35.71961 
41   89 
83.60523   35.71964 
41   89 
84.1566   33.33562 
41   89 
84.15669   33.33562 
41   89 
84.1566   33.33565 
41   89 
85.49316   31.63845 
41   89 
85.49325   31.63845 
41   89 
85.49316   31.63848 
41   89 
86.19404   27.69924 
41   89 
86.19413   27.69924 
41   89 
86.19404   27.69927 
41   89 
85.81724   29.61777 
41   89 
85.81733   29.61777 
41   89 
85.81724   29.6178 
41   89 
85.95811   29.06692 
41   89 
85.95819   29.06692 
41   89 
85.95811   29.06695 
41   89 
86.04374   28.80826 
41   89 
86.04383   28.80826 
41   89 
86.04374   28.80829 
41   89 
86.06009   28.78569 
41   89 
86.06017   28.78569 
41   89 
86.06009   28.78572 
41   89 
86.06759   28.77859 
41   89 
86.06768   28.77859 
41   89 
86.06759   28.77862 
Fit Mean:  86.06759  Size:  28.77859  Code:  1 
> 
> cat("Final Fit Mean: ", nb_fit_mu, " Size: ", nb_fit_size, " Code: ", nb_fit$code, " Try Size: ", try_size, "\n")
Final Fit Mean:  86.06759  Size:  28.77859  Code:  1  Try Size:  100 
> 
> ## Fit failed = reset parameters so graphing and output code can recognize this
> if ((nb_fit_mu < 0) || (nb_fit_size < 0) || (nb_fit$code != 1))
+ {
+   nb_fit_mu = 0
+   nb_fit_size = 0
+ }
> 
> 
> ## things can go wrong with fitting and we can still end up with invalid values
> 
> fit_nb = c()
> included_fract = 0
> if (nb_fit_mu > 0)
+ {
+   end_fract = pnbinom(end_i_for_fits, mu = nb_fit_mu, size=nb_fit_size)
+   start_fract = pnbinom(start_i_for_fits, mu = nb_fit_mu, size=nb_fit_size)
+   included_fract = end_fract-start_fract;
+ 
+   if (included_fract >= 0.01) {
+ 
+     ## Adjust so that we are back in full coords before making fit!!
+     if (num_per_bin > 1) 
+     {
+       nb_fit_mu = nb_fit_mu * num_per_bin
+     }
+     fit_nb = dnbinom(0:max(X$coverage), mu = nb_fit_mu, size=nb_fit_size)*inner_total/included_fract;
+   }
+ }
> 
> ## If an insufficient amount of fit was included, then invalidate it
> if (included_fract < 0.01)
+ {
+   nb_fit_mu = 0
+   nb_fit_size = 0
+ }
> 
> f_p <- function(par) {
+ 
+   lambda = par[1];
+ 
+   if (lambda <= 0)
+   {
+     return(0);
+   }
+   
+ 	total <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{	
+     #cat(i, " ", lambda, "\n");
+ 		dist[i] <- dpois(i, lambda=lambda);
+ 		total <- total + dist[i] 
+ 	}
+ 	#print (total)
+ 
+  	l <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{
+ 		l <- l + ((X.for.fits[i]/inner_total)-(dist[i]/total))^2;
+ 	}
+ 	return(l);
+ }
> 
> 
> ## Fit Poisson 
> ## - allow fit to fail and set all params to zero/empty if that is the case
> 
> p_fit = NULL
> try(suppressWarnings(p_fit<-nlm(f_p, c(m), print.level=this.print.level)))
> 
> fit_p = c()
> if (!is.null(p_fit) && (p_fit$estimate[1] > 0))
+ {
+   #print (nb_fit$estimate[1])
+   p_fit_lambda = p_fit$estimate[1];
+   #print(0:max(X$coverage))
+ 
+   end_fract = ppois(end_i_for_fits, lambda = p_fit_lambda)
+   start_fract = ppois(start_i_for_fits, lambda = p_fit_lambda)
+   included_fract = end_fract-start_fract;
+ 
+   ## Adjust so that we are back in full coords before making fit!!
+   if (num_per_bin > 1) 
+   {
+     p_fit_lambda = p_fit_lambda * num_per_bin
+   }
+   fit_p<-dpois(0:max(X$coverage), lambda = p_fit_lambda)*inner_total/included_fract;
+ }
> 
> 
> ## Graphing
> ##
> ## don't graph very high values with very little coverage
> i<-max_i
> while (i <= length(X$n) && X$n[i]>0.01*max_n)
+ {		
+ 	i <- i+1;
+ }
> graph_end_i <-i
> 
> ## Ths leaves enough room to the right of the peak for the legend
> graph_end_i = max(floor(2.2 * max_i), graph_end_i);
> 
> ## graphics settings
> my_pch = 21
> my_col = "black";
> my_col_censored = "red";
> 
> if (pdf_output == 0) {
+   
+   ## bitmap() requires ghostscript to be installed.
+   ## taa=4, gaa=2 options NOT compatible with earlier R versions!
+   ## units = "px" NOT compatible with even earlier R versions!
+   
+   if(!capabilities(what = "png"))
+   {
+     ## fallback to ghostscript
+     bitmap(plot_file, height=6, width=7, type = "png16m", res = 72, pointsize=18)
+   } else {
+     ## use X11 function, which gives better resolution
+     png(plot_file, height=6, width=7, units ="in", res = 72, pointsize=18)
+     par(family="sans")
+   }
+ } else {
+   pdf(plot_file, height=6, width=7)
+   par(family="sans")
+ }
> 
> par(mar=c(5.5,7.5,3,1.5));
> 
> max_y = 0
> if (plot_poisson) {
+ 	max_y = max(X$n, fit_p, fit_nb)
+ } else {
+ 	max_y = max(X$n, fit_nb)
+ }
> 
> plot(0:10, 0:10, type="n", lty="solid", ylim=c(0, max_y)*1.05, xlim=c(0, graph_end_i), lwd=1, xaxs="i", yaxs="i", axes=F, las=1, main="Coverage Distribution at Unique-Only Positions", xlab="Coverage depth (reads)", ylab="", cex.lab=1.2, cex.axis=1.2)
> 
> mtext(side = 2, text = "Number of reference positions", line = 5.5, cex=1.2)
> 
> sciNotation <- function(x, digits = 1) {
+     if (length(x) > 1) {
+         return(append(sciNotation(x[1]), sciNotation(x[-1])))     
+ 	} 
+     if (!x) return(0) 
+ 
+ 	exponent <- floor(log10(x)) 
+     base <- round(x / 10^exponent, digits)     
+ 	as.expression(substitute(base %*% 10^exponent, list(base = base, exponent = exponent))) 
+ }
> 
> #axis(2, cex.lab=1.2, las=1, cex.axis=1.2, labels=T, at=(0:6)*50000)
> axis(2, cex.lab=1.2, las=1, cex.axis=1.2, at = axTicks(2), labels = sciNotation(axTicks(2), 1))
> axis(1, cex.lab=1.2, cex.axis=1.2, labels=T)
> box()
> 
> #graph the coverage as points
> fit_data <- subset(X, (coverage>=start_i) & (coverage<=end_i) );
> points(fit_data$coverage, fit_data$n, pch=my_pch, col=my_col, bg="white", cex=1.2)
> 
> #graph the censored coverage as red points
> cat(start_i, " ", end_i, "\n", sep="")
41 89
> 
> censored_data <- subset(X, (coverage<start_i) | (coverage>end_i) );
> points(censored_data$coverage, censored_data$n, pch=my_pch, col=my_col_censored, bg="white", cex=1.2)
> 
> #graph the poisson fit IF REQUESTED
> if (plot_poisson) {
+ 	lines(0:max(X$coverage), fit_p, lwd=3, lty="22", col="black");
+ }
> 
> #graph the negative binomial fit
> if (nb_fit_mu > 0) {
+   lines(0:max(X$coverage), fit_nb, lwd=3, col="black");
+ }
> 
> if (plot_poisson) {
+ 	legend("topright", c("Coverage distribution", "Censored data", "Negative binomial", "Poisson"), lty=c("blank","blank","solid","22"), lwd=c(1,1,2,2), pch=c(my_pch, my_pch, -1, -1), col=c("black", "red", "black", "black"), bty="n")
+ } else {
+ 	legend("topright", c("Coverage distribution", "Censored data", "Negative binomial"), lty=c("blank","blank","solid"), lwd=c(1,1,2), pch=c(my_pch, my_pch, -1), col=c("black", "red", "black"), bty="n")
+ }
> 
> dev.off()
null device 
          1 
> 
> ## Fit the marginal value that we use for propagating deletions
> 
> if (nb_fit_mu > 0) {
+   cat(nb_fit_size, " ", nb_fit_mu, "\n")
+   deletion_propagation_coverage = suppressWarnings(qnbinom(deletion_propagation_pr_cutoff, size = nb_fit_size, mu = nb_fit_mu))
+ } else {
+   cat("Fallback to calculating off an estimate of just variance = mu + mu^2/size\n")
+   size_estimate = (1/(v-m))*(m*m)
+   cat("Mu estimate=", m," Size estimate =", size_estimate, "\n")
+   deletion_propagation_coverage = suppressWarnings(qnbinom(deletion_propagation_pr_cutoff, size = size_estimate, mu = m))
+   if (is.na(deletion_propagation_coverage) || is.nan(deletion_propagation_coverage) || (deletion_propagation_coverage < 1)) {
+     cat("Double fallback to calculating as just 10% of the mean\n")
+     deletion_propagation_coverage = m * 0.1
+   }
+ }
28.77859   86.06759 
> 
> #Don't allow one read to indicate non-deleted regions
> if (deletion_propagation_coverage < 1) {
+     deletion_propagation_coverage = 1
+ }
> 
> #This works fine with the negative values
> #If we have both low fit coverage and low straight average coverage then we're deleted...
> if ( (nb_fit_mu <= 3) && (m <= 3) ) {
+   deletion_propagation_coverage = -1
+ }
> 
> #print out statistics
> 
> print(nb_fit_size);
[1] 28.77859
> print(nb_fit_mu);
[1] 86.06759
> 
> print(m)
[1] 66.6055
> print(v)
[1] 457.0049
> print(D)
[1] 6.861369
> 
> print(deletion_propagation_coverage)
[1] 42
> 
> warnings()
> 
