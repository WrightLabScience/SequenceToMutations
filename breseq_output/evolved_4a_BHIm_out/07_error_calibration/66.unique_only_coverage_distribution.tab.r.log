
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ##
> ##
> ## AUTHORS
> ##
> ## Jeffrey E. Barrick <jeffrey.e.barrick@gmail.com>
> ##
> ## LICENSE AND COPYRIGHT
> ##
> ## Copyright (c) 2008-2010 Michigan State University
> ## Copyright (c) 2011-2022 The University of Texas at Austin
> ##
> ## breseq is free software; you can redistribute it and/or modify it under the
> ## terms the GNU General Public License as published by the Free Software
> ## Foundation; either version 1, or (at your option) any later version.
> ##
> ##
> 
> ## Arguments:
> ##   distribution_file=/path/to/input 
> ##   plot_file=/path/to/output 
> ##   deletion_propagation_pr_cutoff=float
> ##   plot_poisson=0 or 1
> ##   pdf_output=0 or 1
> 
> ## Returns these values printed out to output log
> ## 
> ##  1. print(nb_fit_size); # 0 if fit failed
> ##  2. print(nb_fit_mu);   # 0 if fit failed
> ##  3. print(m)q
> ##  4. print(v)
> ##  5. print(D)
> ##  6. print(deletion_propagation_coverage)
> ##     -1 if it was <1 after fitting (implying reference sequence is deleted)
> ##
> 
> plot_poisson = 0;
> pdf_output = 1;
> 
> this.print.level = 0
> #this.print.level = 2
> 
> for (e in commandArgs(TRUE)) {
+   ta = strsplit(e,"=",fixed=TRUE)[[1]]
+   if(length(ta)>1) {
+     temp = ta[2]
+     assign(ta[1],temp)
+     cat("assigned ",ta[1]," the value of |",temp,"|\n")
+   } else {
+     assign(ta[[1]][1],TRUE)
+     cat("assigned ",ta[1]," the value of TRUE\n")
+   }
+ }
assigned  distribution_file  the value of | 4a-_BHI_c50_out/07_error_calibration/66.unique_only_coverage_distribution.tab |
assigned  plot_file  the value of | 4a-_BHI_c50_out/output/calibration/66.unique_coverage.pdf |
assigned  deletion_propagation_pr_cutoff  the value of | 0.00250627 |
> 
> deletion_propagation_pr_cutoff = as.numeric(deletion_propagation_pr_cutoff);
> 
> ## initialize values to be filled in
> nb_fit_mu = 0
> nb_fit_size = 0
> m = 0
> v = 0
> D = 0
> deletion_propagation_coverage = -1
> 
> min_fraction_included_in_nb_fit = 0.01
> 
> #load data
> X<-read.table(distribution_file, header=T)
> 
> #table might be empty
> if (nrow(X) == 0)
+ {
+   #print out statistics
+   
+   print(nb_fit_size);
+   print(nb_fit_mu);
+   
+   print(m)
+   print(v)
+   print(D)
+   
+   print(deletion_propagation_coverage)
+   
+   q()
+ }
> 
> #create the distribution vector and fit
> Y<-rep(X$coverage, X$n)
> m<-mean(Y)
> v<-var(Y)
> D<-v/m
> 
> ###
> ## Smooth the distribution with a moving average window of size 5
> ## so that we can more reliably find it's maximum value
> ###
> 
> ma5 = c(1, 1, 1, 1, 1)/5;
> 
> ## filtering fails if there are too few points
> if (nrow(X) >= 5) {
+   X$ma = filter(X$n, ma5)
+ } else {
+ 	X$ma = X$n
+ }
> 
> i<-0
> max_n <- 0;
> min_i <- max( trunc(m/4), 1 ); #prevents zero for pathological distributions
> max_i <- i;
> for (i in min_i:length(X$ma))
+ {		
+   #cat(i, "\n")
+ 	if (!is.na(X$ma[i]) && (X$ma[i] > max_n))
+ 	{
+ 		max_n = X$ma[i];
+ 		max_i = i;
+ 	}
+ }
> 
> ##
> # Censor data on the right and left of the maximum
> ##
> 
> start_i = max(floor(max_i*0.5), 1);
> end_i = min(ceiling(max_i*1.5), length(X$ma));
> 
> if (start_i == end_i)
+ {
+   print(nb_fit_size);
+   print(nb_fit_mu);
+   
+   print(m)
+   print(v)
+   print(D)
+   
+   print(deletion_propagation_coverage)
+   
+   q()
+ }
> 
> cat("Fitting from coverage of ", start_i, " to ", end_i, ".\n", sep="")
Fitting from coverage of 12 to 32.
> 
> ##
> # Coarse grain so that we are only fitting a number of bins that is 1000-2000
> #
> # The later adjustment for doing the fits this way is to multiply the means
> # of the negative binomial and poisson distributions by the binning number.
> # (The size parameter of the negative binomial doesn't need to be adjusted.)
> ##
> 
> 
> num_per_bin = trunc((end_i - start_i) / 1000)
> 
> if (num_per_bin > 1) 
+ {
+   cat("Coarse-graining for fits\n")
+   start_i_for_fits = trunc(start_i/num_per_bin)
+   end_i_for_fits = ceiling(end_i/num_per_bin)
+   num_bins = end_i - start_i  + 1
+   cat("Fitting from coverage in adjusted bins ", start_i_for_fits, " to ", end_i_for_fits, ".\n", sep="")
+   cat("Number of bins ", num_bins, ". Each bin has ", num_per_bin, " coverage values.\n", sep="")
+ 
+   # Create a new vector where we've added together values in bins
+   X.for.fits = vector("double", end_i_for_fits)
+   for (i in start_i_for_fits:end_i_for_fits)
+   {
+     for (j in 1:num_per_bin)
+     {
+       if (i*num_per_bin+j <= length(X$n))
+       {
+         X.for.fits[i] = X.for.fits[i] + X$n[i*num_per_bin+j]
+       }
+     }
+   }
+ 
+ } else {
+   ## AVOID num_per_bin equalling zero!!
+   X.for.fits = X$n[1:end_i]
+   num_per_bin = 1
+   start_i_for_fits = start_i
+   end_i_for_fits = end_i
+ }
> 
> 
> ##
> # Now perform negative binomial fitting to the censored data
> ##
> 
> inner_total<-0;
> for (i in start_i_for_fits:end_i_for_fits)
+ {
+ 	inner_total = inner_total + X.for.fits[i]; 
+ }
> # Yes: it's correct to use X here because we want the overall total total
> total_total<-sum(X$n);
> 
> ## let's preconstruct these for speed
> dist = vector("double", end_i_for_fits)
> 
> f_nb <- function(par) {
+ 
+ 	mu = par[1];
+ 	size = par[2];
+ 
+   if ((mu <= 0) || (size <= 0))
+   {
+     return(0);
+   }
+   
+   cat(start_i_for_fits, " ", end_i_for_fits, "\n");
+   cat(mu, " ", size, "\n");
+   
+ 	dist<-c()
+ 	total <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{	
+ 		dist[i] <- dnbinom(i, size=size, mu=mu);
+ 		total <- total + dist[i] 
+ 	}
+ 	#print (mu, size)
+ 
+  	l <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{
+ 		l <- l + ((X.for.fits[i]/inner_total)-(dist[i]/total))^2;
+ 	}
+ 	return(l);
+ }
> 
> 
> 
> ## Fit negative binomial 
> ## - allow fit to fail and set all params to zero/empty if that is the case
> nb_fit = NULL
> ## as.numeric prevents overflow in sums involving integers
> mean_estimate = sum((as.numeric(1:end_i_for_fits)*as.numeric(X.for.fits)))/sum(as.numeric(X.for.fits))
> 
> nb_fit_mu = -1
> nb_fit_size = -1
> try_size = 100000
> try_means_index = 1
> #This is a list of different means to test <-  sometimes the actual mean doesn't lead to a fit
> try_means = c(mean_estimate, 
+               end_i_for_fits, 
+               start_i_for_fits, 
+               1*(end_i_for_fits + start_i_for_fits)/4,
+               2*(end_i_for_fits + start_i_for_fits)/4,
+               3*(end_i_for_fits + start_i_for_fits)/4
+               )
>               
>               
> nb_fit = c()
> 
> while ( ((nb_fit_mu < 0) || (nb_fit_size < 0) || (nb_fit$code != 1)) && (try_size > 0.001) && (try_means_index <= length(try_means)))
+ {
+   try_size = try_size / 10
+   try_mean = try_means[try_means_index]
+ 
+   ## SIZE ESTIMATE from the censored data can be negative, so try various values instead
+   cat("Try Mean: ", try_mean, " Size: ", try_size, "\n")
+ 
+   try( suppressWarnings(nb_fit<-nlm(f_nb, c(try_mean, try_size), iterlim=1000, print.level=this.print.level)) )
+ 
+   nb_fit_mu = nb_fit$estimate[1];
+   nb_fit_size = nb_fit$estimate[2];
+ 
+   cat("Fit Mean: ", nb_fit_mu, " Size: ", nb_fit_size, " Code: ", nb_fit$code, "\n")
+   
+   if (try_size <= 0.001) {
+     try_size = 100000
+     try_means_index = try_means_index + 1
+   }
+ }
Try Mean:  22.1769  Size:  10000 
12   32 
22.1769   10000 
12   32 
22.1769   10000 
12   32 
22.17692   10000 
12   32 
22.1769   10000.01 
12   32 
22.18476   10000 
12   32 
22.18478   10000 
12   32 
22.18476   10000.01 
12   32 
25.46719   10000 
12   32 
25.46722   10000 
12   32 
25.46719   10000.01 
12   32 
25.22069   10000 
12   32 
25.22072   10000 
12   32 
25.22069   10000.01 
12   32 
25.21249   10000 
12   32 
25.21252   10000 
12   32 
25.21249   10000.01 
12   32 
25.21257   10000 
12   32 
25.2126   10000 
12   32 
25.21257   10000.01 
12   32 
25.21257   10000 
12   32 
25.2151   10000 
12   32 
25.21005   10000 
12   32 
25.21257   10001 
12   32 
25.21257   9999 
12   32 
25.21259   10000 
12   32 
25.21511   10000 
12   32 
25.21007   10000 
12   32 
25.21259   10001 
12   32 
25.21259   9999 
Fit Mean:  25.21259  Size:  10000  Code:  2 
Try Mean:  22.1769  Size:  1000 
12   32 
22.1769   1000 
12   32 
22.1769   1000 
12   32 
22.17692   1000 
12   32 
22.1769   1000.001 
12   32 
22.18464   1000 
12   32 
22.18466   1000 
12   32 
22.18464   1000.001 
12   32 
25.46447   1000 
12   32 
25.46449   1000 
12   32 
25.46447   1000.001 
12   32 
25.24329   1000 
12   32 
25.24332   1000 
12   32 
25.24329   1000.001 
12   32 
25.23464   1000 
12   32 
25.23467   1000 
12   32 
25.23464   1000.001 
12   32 
25.23472   1000 
12   32 
25.23475   1000 
12   32 
25.23472   1000.001 
12   32 
25.23473   1000 
12   32 
25.23476   1000 
12   32 
25.23473   1000.001 
Fit Mean:  25.23473  Size:  1000  Code:  2 
Try Mean:  22.1769  Size:  100 
12   32 
22.1769   100 
12   32 
22.1769   100 
12   32 
22.17692   100 
12   32 
22.1769   100.0001 
12   32 
22.18362   100 
12   32 
22.18364   100 
12   32 
22.18362   100.0001 
12   32 
25.47061   100.0153 
12   32 
25.47063   100.0153 
12   32 
25.47061   100.0154 
12   32 
25.47647   100.0195 
12   32 
25.4765   100.0195 
12   32 
25.47647   100.0196 
12   32 
25.48019   100.0283 
12   32 
25.48022   100.0283 
12   32 
25.48019   100.0284 
12   32 
25.48751   100.064 
12   32 
25.48753   100.064 
12   32 
25.48751   100.0641 
12   32 
25.49834   100.1633 
12   32 
25.49837   100.1633 
12   32 
25.49834   100.1634 
12   32 
25.51588   100.4491 
12   32 
25.5159   100.4491 
12   32 
25.51588   100.4492 
12   32 
25.54238   101.2121 
12   32 
25.54241   101.2121 
12   32 
25.54238   101.2122 
12   32 
25.58055   103.2076 
12   32 
25.58057   103.2076 
12   32 
25.58055   103.2077 
12   32 
25.62775   108.1724 
12   32 
25.62777   108.1724 
12   32 
25.62775   108.1725 
12   32 
25.66718   119.9069 
12   32 
25.66721   119.9069 
12   32 
25.66718   119.9071 
12   32 
25.65325   146.7596 
12   32 
25.65328   146.7596 
12   32 
25.65325   146.7598 
12   32 
25.50328   205.0653 
12   32 
25.50331   205.0653 
12   32 
25.50328   205.0655 
12   32 
25.27069   286.1811 
12   32 
25.27071   286.1811 
12   32 
25.27069   286.1814 
12   32 
25.19133   332.7399 
12   32 
25.19136   332.7399 
12   32 
25.19133   332.7403 
12   32 
25.15761   388.278 
12   32 
25.15763   388.278 
12   32 
25.15761   388.2784 
12   32 
25.16051   499.2927 
12   32 
25.16053   499.2927 
12   32 
25.16051   499.2932 
12   32 
25.20709   654.771 
12   32 
25.20712   654.771 
12   32 
25.20709   654.7716 
12   32 
25.24645   840.0796 
12   32 
25.24648   840.0796 
12   32 
25.24645   840.0804 
12   32 
25.25984   1066.215 
12   32 
25.25987   1066.215 
12   32 
25.25984   1066.216 
12   32 
25.25101   1402.126 
12   32 
25.25104   1402.126 
12   32 
25.25101   1402.128 
12   32 
25.22845   1869.165 
12   32 
25.22847   1869.165 
12   32 
25.22845   1869.167 
12   32 
25.20965   2447.668 
12   32 
25.20967   2447.668 
12   32 
25.20965   2447.67 
12   32 
25.20134   3177.938 
12   32 
25.20137   3177.938 
12   32 
25.20134   3177.941 
12   32 
25.20265   4181.053 
12   32 
25.20267   4181.053 
12   32 
25.20265   4181.058 
12   32 
25.20932   5532.53 
12   32 
25.20934   5532.53 
12   32 
25.20932   5532.535 
12   32 
25.21511   7277.824 
12   32 
25.21513   7277.824 
12   32 
25.21511   7277.831 
12   32 
25.2171   9567.141 
12   32 
25.21712   9567.141 
12   32 
25.2171   9567.15 
12   32 
25.21547   12664.54 
12   32 
25.2155   12664.54 
12   32 
25.21547   12664.56 
12   32 
25.2122   16796.85 
12   32 
25.21223   16796.85 
12   32 
25.2122   16796.86 
12   32 
25.20954   22187.67 
12   32 
25.20956   22187.67 
12   32 
25.20954   22187.69 
12   32 
25.20851   29291.71 
12   32 
25.20854   29291.71 
12   32 
25.20851   29291.74 
12   32 
25.20897   38767.33 
12   32 
25.20899   38767.33 
12   32 
25.20897   38767.37 
12   32 
25.21006   51360.81 
12   32 
25.21009   51360.81 
12   32 
25.21006   51360.86 
12   32 
25.21092   67956.4 
12   32 
25.21095   67956.4 
12   32 
25.21092   67956.46 
12   32 
25.21116   89841.2 
12   32 
25.21118   89841.2 
12   32 
25.21116   89841.29 
12   32 
25.21085   119149.2 
12   32 
25.21087   119149.2 
12   32 
25.21085   119149.4 
12   32 
25.21034   157509.8 
12   32 
25.21037   157509.8 
12   32 
25.21034   157510 
12   32 
25.20997   207971.9 
12   32 
25.20999   207971.9 
12   32 
25.20997   207972.1 
12   32 
25.20985   276285.9 
12   32 
25.20988   276285.9 
12   32 
25.20985   276286.2 
12   32 
25.20996   362601.6 
12   32 
25.20998   362601.6 
12   32 
25.20996   362601.9 
12   32 
25.21011   465031.1 
12   32 
25.21013   465031.1 
12   32 
25.21011   465031.6 
12   32 
25.2102   567460.7 
12   32 
25.21023   567460.7 
12   32 
25.2102   567461.3 
12   32 
25.21024   669890.3 
12   32 
25.21026   669890.3 
12   32 
25.21024   669890.9 
12   32 
25.21024   772319.8 
12   32 
25.21026   772319.8 
12   32 
25.21024   772320.6 
12   32 
25.21023   874749.4 
12   32 
25.21025   874749.4 
12   32 
25.21023   874750.3 
Fit Mean:  25.21023  Size:  874749.4  Code:  5 
Try Mean:  22.1769  Size:  10 
12   32 
22.1769   10 
12   32 
22.1769   10 
12   32 
22.17692   10 
12   32 
22.1769   10.00001 
12   32 
22.18027   10.0005 
12   32 
22.18029   10.0005 
12   32 
22.18027   10.00051 
12   32 
27.47854   12.59645 
12   32 
27.47857   12.59645 
12   32 
27.47854   12.59646 
12   32 
28.3944   13.55689 
12   32 
28.39442   13.55689 
12   32 
28.3944   13.5569 
12   32 
28.46493   14.0152 
12   32 
28.46496   14.0152 
12   32 
28.46493   14.01521 
12   32 
28.26732   16.66449 
12   32 
28.26735   16.66449 
12   32 
28.26732   16.66451 
12   32 
26.73263   25.51121 
12   32 
26.73266   25.51121 
12   32 
26.73263   25.51124 
12   32 
23.32196   46.65967 
12   32 
25.99865   30.06243 
12   32 
25.99867   30.06243 
12   32 
25.99865   30.06246 
12   32 
25.55911   35.35297 
12   32 
25.55914   35.35297 
12   32 
25.55911   35.353 
12   32 
25.46783   41.901 
12   32 
25.46785   41.901 
12   32 
25.46783   41.90104 
12   32 
25.65442   56.62391 
12   32 
25.65444   56.62391 
12   32 
25.65442   56.62397 
12   32 
25.55085   73.14166 
12   32 
25.55087   73.14166 
12   32 
25.55085   73.14173 
12   32 
25.40567   98.29912 
12   32 
25.40569   98.29912 
12   32 
25.40567   98.29922 
12   32 
25.38732   128.6021 
12   32 
25.38735   128.6021 
12   32 
25.38732   128.6022 
12   32 
25.31896   169.1155 
12   32 
25.31899   169.1155 
12   32 
25.31896   169.1157 
12   32 
25.32289   221.6049 
12   32 
25.32291   221.6049 
12   32 
25.32289   221.6051 
12   32 
25.25736   291.9211 
12   32 
25.25739   291.9211 
12   32 
25.25736   291.9214 
12   32 
25.28717   384.5819 
12   32 
25.28719   384.5819 
12   32 
25.28717   384.5823 
12   32 
25.23218   508.4729 
12   32 
25.23221   508.4729 
12   32 
25.23218   508.4734 
12   32 
25.2443   668.5194 
12   32 
25.24433   668.5194 
12   32 
25.2443   668.52 
12   32 
25.23005   879.1353 
12   32 
25.23007   879.1353 
12   32 
25.23005   879.1362 
12   32 
25.22747   1158.607 
12   32 
25.2275   1158.607 
12   32 
25.22747   1158.608 
12   32 
25.22229   1527.294 
12   32 
25.22232   1527.294 
12   32 
25.22229   1527.296 
12   32 
25.21978   2016.121 
12   32 
25.21981   2016.121 
12   32 
25.21978   2016.123 
12   32 
25.2172   2662.889 
12   32 
25.21722   2662.889 
12   32 
25.2172   2662.892 
12   32 
25.21559   3519.793 
12   32 
25.21562   3519.793 
12   32 
25.21559   3519.796 
12   32 
25.21419   4654.551 
12   32 
25.21422   4654.551 
12   32 
25.21419   4654.556 
12   32 
25.21324   6157.797 
12   32 
25.21326   6157.797 
12   32 
25.21324   6157.803 
12   32 
25.21246   8148.997 
12   32 
25.21248   8148.997 
12   32 
25.21246   8149.005 
12   32 
25.2119   10786.69 
12   32 
25.21192   10786.69 
12   32 
25.2119   10786.7 
12   32 
25.21146   14280.85 
12   32 
25.21149   14280.85 
12   32 
25.21146   14280.86 
12   32 
25.21114   18909.08 
12   32 
25.21117   18909.08 
12   32 
25.21114   18909.1 
12   32 
25.21089   25040.03 
12   32 
25.21092   25040.03 
12   32 
25.21089   25040.05 
12   32 
25.21071   33162.56 
12   32 
25.21073   33162.56 
12   32 
25.21071   33162.59 
12   32 
25.21057   43920.54 
12   32 
25.21059   43920.54 
12   32 
25.21057   43920.59 
12   32 
25.21046   58201.39 
12   32 
25.21049   58201.39 
12   32 
25.21046   58201.45 
12   32 
25.21038   77024.08 
12   32 
25.21041   77024.08 
12   32 
25.21038   77024.16 
12   32 
25.21032   101351.3 
12   32 
25.21035   101351.3 
12   32 
25.21032   101351.4 
12   32 
25.21029   125678.6 
12   32 
25.21032   125678.6 
12   32 
25.21029   125678.7 
12   32 
25.21027   150005.8 
12   32 
25.21029   150005.8 
12   32 
25.21027   150006 
12   32 
25.21025   174333.1 
12   32 
25.21028   174333.1 
12   32 
25.21025   174333.2 
12   32 
25.21024   198660.3 
12   32 
25.21026   198660.3 
12   32 
25.21024   198660.5 
Fit Mean:  25.21024  Size:  198660.3  Code:  5 
Try Mean:  22.1769  Size:  1 
12   32 
22.1769   1 
12   32 
22.1769   1 
12   32 
22.17692   1 
12   32 
22.1769   1.000001 
12   32 
22.17759   1.002725 
12   32 
22.17762   1.002725 
12   32 
22.17759   1.002726 
12   32 
47.72684   35.73303 
12   32 
30.33292   12.08865 
12   32 
30.33295   12.08865 
12   32 
30.33292   12.08866 
12   32 
29.34473   11.15848 
12   32 
29.34476   11.15848 
12   32 
29.34473   11.15849 
12   32 
29.49043   11.5364 
12   32 
29.49046   11.5364 
12   32 
29.49043   11.53641 
12   32 
29.48071   12.46485 
12   32 
29.48074   12.46485 
12   32 
29.48071   12.46487 
12   32 
28.82895   15.53875 
12   32 
28.82898   15.53875 
12   32 
28.82895   15.53876 
12   32 
18.5851   54.84874 
12   32 
27.80457   19.46975 
12   32 
27.80459   19.46975 
12   32 
27.80457   19.46977 
Fit Mean:  -308.8657  Size:  1339.404  Code:  1 
Try Mean:  22.1769  Size:  0.1 
12   32 
22.1769   0.1 
12   32 
22.1769   0.1 
12   32 
22.17692   0.1 
12   32 
22.1769   0.100001 
12   32 
22.17697   0.1034356 
12   32 
22.177   0.1034356 
12   32 
22.17697   0.1034366 
12   32 
25.8709   7.117891 
12   32 
25.87093   7.117891 
12   32 
25.8709   7.117898 
12   32 
32.20273   15.9944 
12   32 
32.20277   15.9944 
12   32 
32.20273   15.99442 
12   32 
29.34403   12.28258 
12   32 
29.34405   12.28258 
12   32 
29.34403   12.28259 
12   32 
29.22728   12.34425 
12   32 
29.22731   12.34425 
12   32 
29.22728   12.34426 
12   32 
26.18686   15.75397 
12   32 
26.18689   15.75397 
12   32 
26.18686   15.75399 
12   32 
27.54115   15.22962 
12   32 
27.54118   15.22962 
12   32 
27.54115   15.22964 
12   32 
27.31874   16.88857 
12   32 
27.31876   16.88857 
12   32 
27.31874   16.88858 
12   32 
25.624   26.9595 
12   32 
25.62403   26.9595 
12   32 
25.624   26.95953 
12   32 
26.10546   31.73571 
12   32 
26.10548   31.73571 
12   32 
26.10546   31.73574 
12   32 
25.82168   43.54454 
12   32 
25.8217   43.54454 
12   32 
25.82168   43.54459 
12   32 
25.53289   58.62579 
12   32 
25.53292   58.62579 
12   32 
25.53289   58.62585 
12   32 
25.56695   78.03256 
12   32 
25.56698   78.03256 
12   32 
25.56695   78.03264 
12   32 
25.32291   104.7181 
12   32 
25.32293   104.7181 
12   32 
25.32291   104.7182 
12   32 
25.40035   137.7905 
12   32 
25.40038   137.7905 
12   32 
25.40035   137.7907 
12   32 
25.31048   180.8232 
12   32 
25.31051   180.8232 
12   32 
25.31048   180.8233 
12   32 
25.29885   237.4395 
12   32 
25.29888   237.4395 
12   32 
25.29885   237.4398 
12   32 
25.27208   310.5644 
12   32 
25.27211   310.5644 
12   32 
25.27208   310.5647 
12   32 
25.26147   407.4003 
12   32 
25.2615   407.4003 
12   32 
25.26147   407.4007 
12   32 
25.24575   534.648 
12   32 
25.24577   534.648 
12   32 
25.24575   534.6485 
12   32 
25.24032   702.7612 
12   32 
25.24035   702.7612 
12   32 
25.24032   702.7619 
12   32 
25.22989   924.9607 
12   32 
25.22991   924.9607 
12   32 
25.22989   924.9616 
12   32 
25.22834   1218.804 
12   32 
25.22836   1218.804 
12   32 
25.22834   1218.805 
12   32 
25.22041   1608.14 
12   32 
25.22044   1608.14 
12   32 
25.22041   1608.141 
12   32 
25.22168   2123.539 
12   32 
25.22171   2123.539 
12   32 
25.22168   2123.541 
12   32 
25.21473   2807.625 
12   32 
25.21475   2807.625 
12   32 
25.21473   2807.628 
12   32 
25.21796   3714.258 
12   32 
25.21798   3714.258 
12   32 
25.21796   3714.262 
12   32 
25.21158   4919.329 
12   32 
25.21161   4919.329 
12   32 
25.21158   4919.334 
12   32 
25.21539   6517.288 
12   32 
25.21541   6517.288 
12   32 
25.21539   6517.294 
12   32 
25.21052   8638.61 
12   32 
25.21055   8638.61 
12   32 
25.21052   8638.619 
12   32 
25.21314   11447.45 
12   32 
25.21316   11447.45 
12   32 
25.21314   11447.46 
12   32 
25.21051   15166.5 
12   32 
25.21054   15166.5 
12   32 
25.21051   15166.52 
12   32 
25.21163   20090.75 
12   32 
25.21166   20090.75 
12   32 
25.21163   20090.77 
12   32 
25.21051   26612.12 
12   32 
25.21054   26612.12 
12   32 
25.21051   26612.15 
12   32 
25.21088   35251.65 
12   32 
25.2109   35251.65 
12   32 
25.21088   35251.68 
12   32 
25.21042   46676.3 
12   32 
25.21045   46676.3 
12   32 
25.21042   46676.35 
12   32 
25.21051   61848.52 
12   32 
25.21054   61848.52 
12   32 
25.21051   61848.59 
12   32 
25.21033   81895.23 
12   32 
25.21035   81895.23 
12   32 
25.21033   81895.32 
12   32 
25.21034   104072.4 
12   32 
25.21036   104072.4 
12   32 
25.21034   104072.5 
12   32 
25.21029   126249.5 
12   32 
25.21031   126249.5 
12   32 
25.21029   126249.6 
12   32 
25.21027   148426.6 
12   32 
25.2103   148426.6 
12   32 
25.21027   148426.7 
12   32 
25.21025   170603.7 
12   32 
25.21028   170603.7 
12   32 
25.21025   170603.9 
12   32 
25.21024   192780.8 
12   32 
25.21027   192780.8 
12   32 
25.21024   192781 
Fit Mean:  25.21024  Size:  192780.8  Code:  5 
Try Mean:  22.1769  Size:  0.01 
12   32 
22.1769   0.01 
12   32 
22.1769   0.01 
12   32 
22.17692   0.01 
12   32 
22.1769   0.010001 
12   32 
22.1769   0.01351939 
12   32 
22.17693   0.01351939 
12   32 
22.1769   0.01352039 
12   32 
25.35835   6.424254 
12   32 
25.35838   6.424254 
12   32 
25.35835   6.42426 
12   32 
32.48329   16.6884 
12   32 
32.48332   16.6884 
12   32 
32.48329   16.68842 
12   32 
29.29063   12.40553 
12   32 
29.29066   12.40553 
12   32 
29.29063   12.40554 
12   32 
29.18457   12.46815 
12   32 
29.1846   12.46815 
12   32 
29.18457   12.46816 
12   32 
25.85969   16.4319 
12   32 
25.85971   16.4319 
12   32 
25.85969   16.43192 
12   32 
27.62515   15.33439 
12   32 
27.62517   15.33439 
12   32 
27.62515   15.33441 
12   32 
27.26778   16.91055 
12   32 
27.2678   16.91055 
12   32 
27.26778   16.91056 
12   32 
25.42317   27.12268 
12   32 
25.4232   27.12268 
12   32 
25.42317   27.1227 
12   32 
26.12934   29.84996 
12   32 
26.12937   29.84996 
12   32 
26.12934   29.84999 
12   32 
26.06743   38.54276 
12   32 
26.06746   38.54276 
12   32 
26.06743   38.5428 
12   32 
25.54648   53.9483 
12   32 
25.5465   53.9483 
12   32 
25.54648   53.94836 
12   32 
25.45491   69.99621 
12   32 
25.45494   69.99621 
12   32 
25.45491   69.99628 
12   32 
25.47851   92.01573 
12   32 
25.47854   92.01573 
12   32 
25.47851   92.01582 
12   32 
25.34174   121.9582 
12   32 
25.34177   121.9582 
12   32 
25.34174   121.9583 
12   32 
25.3805   160.417 
12   32 
25.38053   160.417 
12   32 
25.3805   160.4171 
12   32 
25.27351   212.2279 
12   32 
25.27353   212.2279 
12   32 
25.27351   212.2281 
12   32 
25.296   278.3418 
12   32 
25.29602   278.3418 
12   32 
25.296   278.3421 
12   32 
25.2575   364.9968 
12   32 
25.25752   364.9968 
12   32 
25.2575   364.9971 
12   32 
25.2551   479.2807 
12   32 
25.25513   479.2807 
12   32 
25.2551   479.2812 
12   32 
25.23899   629.3679 
12   32 
25.23901   629.3679 
12   32 
25.23899   629.3685 
12   32 
25.23552   828.0375 
12   32 
25.23555   828.0375 
12   32 
25.23552   828.0383 
12   32 
25.22682   1090.398 
12   32 
25.22685   1090.398 
12   32 
25.22682   1090.399 
12   32 
25.22471   1437.715 
12   32 
25.22473   1437.715 
12   32 
25.22471   1437.716 
12   32 
25.2196   1897.38 
12   32 
25.21962   1897.38 
12   32 
25.2196   1897.382 
12   32 
25.21857   2506.065 
12   32 
25.21859   2506.065 
12   32 
25.21857   2506.068 
12   32 
25.21542   3312.245 
12   32 
25.21545   3312.245 
12   32 
25.21542   3312.248 
12   32 
25.21506   4379.982 
12   32 
25.21508   4379.982 
12   32 
25.21506   4379.986 
12   32 
25.21305   5794.516 
12   32 
25.21307   5794.516 
12   32 
25.21305   5794.522 
12   32 
25.21303   7668.113 
12   32 
25.21306   7668.113 
12   32 
25.21303   7668.121 
12   32 
25.21171   10150.56 
12   32 
25.21174   10150.56 
12   32 
25.21171   10150.57 
12   32 
25.21186   13438.43 
12   32 
25.21189   13438.43 
12   32 
25.21186   13438.44 
12   32 
25.21097   17795.44 
12   32 
25.21099   17795.44 
12   32 
25.21097   17795.46 
12   32 
25.21118   23566.66 
12   32 
25.2112   23566.66 
12   32 
25.21118   23566.68 
12   32 
25.21056   31208.47 
12   32 
25.21059   31208.47 
12   32 
25.21056   31208.5 
12   32 
25.21077   41345.65 
12   32 
25.2108   41345.65 
12   32 
25.21077   41345.69 
12   32 
25.21034   54768.81 
12   32 
25.21037   54768.81 
12   32 
25.21034   54768.87 
12   32 
25.21053   72470.62 
12   32 
25.21056   72470.62 
12   32 
25.21053   72470.69 
12   32 
25.21024   94647.52 
12   32 
25.21027   94647.52 
12   32 
25.21024   94647.61 
12   32 
25.21034   116824.4 
12   32 
25.21037   116824.4 
12   32 
25.21034   116824.5 
Fit Mean:  25.21034  Size:  116824.4  Code:  1 
> 
> cat("Final Fit Mean: ", nb_fit_mu, " Size: ", nb_fit_size, " Code: ", nb_fit$code, " Try Size: ", try_size, "\n")
Final Fit Mean:  25.21034  Size:  116824.4  Code:  1  Try Size:  0.01 
> 
> ## Fit failed = reset parameters so graphing and output code can recognize this
> if ((nb_fit_mu < 0) || (nb_fit_size < 0) || (nb_fit$code != 1))
+ {
+   nb_fit_mu = 0
+   nb_fit_size = 0
+ }
> 
> 
> ## things can go wrong with fitting and we can still end up with invalid values
> 
> fit_nb = c()
> included_fract = 0
> if (nb_fit_mu > 0)
+ {
+   end_fract = pnbinom(end_i_for_fits, mu = nb_fit_mu, size=nb_fit_size)
+   start_fract = pnbinom(start_i_for_fits, mu = nb_fit_mu, size=nb_fit_size)
+   included_fract = end_fract-start_fract;
+ 
+   if (included_fract >= 0.01) {
+ 
+     ## Adjust so that we are back in full coords before making fit!!
+     if (num_per_bin > 1) 
+     {
+       nb_fit_mu = nb_fit_mu * num_per_bin
+     }
+     fit_nb = dnbinom(0:max(X$coverage), mu = nb_fit_mu, size=nb_fit_size)*inner_total/included_fract;
+   }
+ }
> 
> ## If an insufficient amount of fit was included, then invalidate it
> if (included_fract < 0.01)
+ {
+   nb_fit_mu = 0
+   nb_fit_size = 0
+ }
> 
> f_p <- function(par) {
+ 
+   lambda = par[1];
+ 
+   if (lambda <= 0)
+   {
+     return(0);
+   }
+   
+ 	total <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{	
+     #cat(i, " ", lambda, "\n");
+ 		dist[i] <- dpois(i, lambda=lambda);
+ 		total <- total + dist[i] 
+ 	}
+ 	#print (total)
+ 
+  	l <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{
+ 		l <- l + ((X.for.fits[i]/inner_total)-(dist[i]/total))^2;
+ 	}
+ 	return(l);
+ }
> 
> 
> ## Fit Poisson 
> ## - allow fit to fail and set all params to zero/empty if that is the case
> 
> p_fit = NULL
> try(suppressWarnings(p_fit<-nlm(f_p, c(m), print.level=this.print.level)))
> 
> fit_p = c()
> if (!is.null(p_fit) && (p_fit$estimate[1] > 0))
+ {
+   #print (nb_fit$estimate[1])
+   p_fit_lambda = p_fit$estimate[1];
+   #print(0:max(X$coverage))
+ 
+   end_fract = ppois(end_i_for_fits, lambda = p_fit_lambda)
+   start_fract = ppois(start_i_for_fits, lambda = p_fit_lambda)
+   included_fract = end_fract-start_fract;
+ 
+   ## Adjust so that we are back in full coords before making fit!!
+   if (num_per_bin > 1) 
+   {
+     p_fit_lambda = p_fit_lambda * num_per_bin
+   }
+   fit_p<-dpois(0:max(X$coverage), lambda = p_fit_lambda)*inner_total/included_fract;
+ }
> 
> 
> ## Graphing
> ##
> ## don't graph very high values with very little coverage
> i<-max_i
> while (i <= length(X$n) && X$n[i]>0.01*max_n)
+ {		
+ 	i <- i+1;
+ }
> graph_end_i <-i
> 
> ## Ths leaves enough room to the right of the peak for the legend
> graph_end_i = max(floor(2.2 * max_i), graph_end_i);
> 
> ## graphics settings
> my_pch = 21
> my_col = "black";
> my_col_censored = "red";
> 
> if (pdf_output == 0) {
+   
+   ## bitmap() requires ghostscript to be installed.
+   ## taa=4, gaa=2 options NOT compatible with earlier R versions!
+   ## units = "px" NOT compatible with even earlier R versions!
+   
+   if(!capabilities(what = "png"))
+   {
+     ## fallback to ghostscript
+     bitmap(plot_file, height=6, width=7, type = "png16m", res = 72, pointsize=18)
+   } else {
+     ## use X11 function, which gives better resolution
+     png(plot_file, height=6, width=7, units ="in", res = 72, pointsize=18)
+     par(family="sans")
+   }
+ } else {
+   pdf(plot_file, height=6, width=7)
+   par(family="sans")
+ }
> 
> par(mar=c(5.5,7.5,3,1.5));
> 
> max_y = 0
> if (plot_poisson) {
+ 	max_y = max(X$n, fit_p, fit_nb)
+ } else {
+ 	max_y = max(X$n, fit_nb)
+ }
> 
> plot(0:10, 0:10, type="n", lty="solid", ylim=c(0, max_y)*1.05, xlim=c(0, graph_end_i), lwd=1, xaxs="i", yaxs="i", axes=F, las=1, main="Coverage Distribution at Unique-Only Positions", xlab="Coverage depth (reads)", ylab="", cex.lab=1.2, cex.axis=1.2)
> 
> mtext(side = 2, text = "Number of reference positions", line = 5.5, cex=1.2)
> 
> sciNotation <- function(x, digits = 1) {
+     if (length(x) > 1) {
+         return(append(sciNotation(x[1]), sciNotation(x[-1])))     
+ 	} 
+     if (!x) return(0) 
+ 
+ 	exponent <- floor(log10(x)) 
+     base <- round(x / 10^exponent, digits)     
+ 	as.expression(substitute(base %*% 10^exponent, list(base = base, exponent = exponent))) 
+ }
> 
> #axis(2, cex.lab=1.2, las=1, cex.axis=1.2, labels=T, at=(0:6)*50000)
> axis(2, cex.lab=1.2, las=1, cex.axis=1.2, at = axTicks(2), labels = sciNotation(axTicks(2), 1))
> axis(1, cex.lab=1.2, cex.axis=1.2, labels=T)
> box()
> 
> #graph the coverage as points
> fit_data <- subset(X, (coverage>=start_i) & (coverage<=end_i) );
> points(fit_data$coverage, fit_data$n, pch=my_pch, col=my_col, bg="white", cex=1.2)
> 
> #graph the censored coverage as red points
> cat(start_i, " ", end_i, "\n", sep="")
12 32
> 
> censored_data <- subset(X, (coverage<start_i) | (coverage>end_i) );
> points(censored_data$coverage, censored_data$n, pch=my_pch, col=my_col_censored, bg="white", cex=1.2)
> 
> #graph the poisson fit IF REQUESTED
> if (plot_poisson) {
+ 	lines(0:max(X$coverage), fit_p, lwd=3, lty="22", col="black");
+ }
> 
> #graph the negative binomial fit
> if (nb_fit_mu > 0) {
+   lines(0:max(X$coverage), fit_nb, lwd=3, col="black");
+ }
> 
> if (plot_poisson) {
+ 	legend("topright", c("Coverage distribution", "Censored data", "Negative binomial", "Poisson"), lty=c("blank","blank","solid","22"), lwd=c(1,1,2,2), pch=c(my_pch, my_pch, -1, -1), col=c("black", "red", "black", "black"), bty="n")
+ } else {
+ 	legend("topright", c("Coverage distribution", "Censored data", "Negative binomial"), lty=c("blank","blank","solid"), lwd=c(1,1,2), pch=c(my_pch, my_pch, -1), col=c("black", "red", "black"), bty="n")
+ }
> 
> dev.off()
null device 
          1 
> 
> ## Fit the marginal value that we use for propagating deletions
> 
> if (nb_fit_mu > 0) {
+   cat(nb_fit_size, " ", nb_fit_mu, "\n")
+   deletion_propagation_coverage = suppressWarnings(qnbinom(deletion_propagation_pr_cutoff, size = nb_fit_size, mu = nb_fit_mu))
+ } else {
+   cat("Fallback to calculating off an estimate of just variance = mu + mu^2/size\n")
+   size_estimate = (1/(v-m))*(m*m)
+   cat("Mu estimate=", m," Size estimate =", size_estimate, "\n")
+   deletion_propagation_coverage = suppressWarnings(qnbinom(deletion_propagation_pr_cutoff, size = size_estimate, mu = m))
+   if (is.na(deletion_propagation_coverage) || is.nan(deletion_propagation_coverage) || (deletion_propagation_coverage < 1)) {
+     cat("Double fallback to calculating as just 10% of the mean\n")
+     deletion_propagation_coverage = m * 0.1
+   }
+ }
116824.4   25.21034 
> 
> #Don't allow one read to indicate non-deleted regions
> if (deletion_propagation_coverage < 1) {
+     deletion_propagation_coverage = 1
+ }
> 
> #This works fine with the negative values
> #If we have both low fit coverage and low straight average coverage then we're deleted...
> if ( (nb_fit_mu <= 3) && (m <= 3) ) {
+   deletion_propagation_coverage = -1
+ }
> 
> #print out statistics
> 
> print(nb_fit_size);
[1] 116824.4
> print(nb_fit_mu);
[1] 25.21034
> 
> print(m)
[1] 22.1769
> print(v)
[1] 50.68961
> print(D)
[1] 2.285695
> 
> print(deletion_propagation_coverage)
[1] 12
> 
> warnings()
> 
