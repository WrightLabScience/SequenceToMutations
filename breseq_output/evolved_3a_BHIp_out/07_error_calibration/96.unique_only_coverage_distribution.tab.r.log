
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ##
> ##
> ## AUTHORS
> ##
> ## Jeffrey E. Barrick <jeffrey.e.barrick@gmail.com>
> ##
> ## LICENSE AND COPYRIGHT
> ##
> ## Copyright (c) 2008-2010 Michigan State University
> ## Copyright (c) 2011-2022 The University of Texas at Austin
> ##
> ## breseq is free software; you can redistribute it and/or modify it under the
> ## terms the GNU General Public License as published by the Free Software
> ## Foundation; either version 1, or (at your option) any later version.
> ##
> ##
> 
> ## Arguments:
> ##   distribution_file=/path/to/input 
> ##   plot_file=/path/to/output 
> ##   deletion_propagation_pr_cutoff=float
> ##   plot_poisson=0 or 1
> ##   pdf_output=0 or 1
> 
> ## Returns these values printed out to output log
> ## 
> ##  1. print(nb_fit_size); # 0 if fit failed
> ##  2. print(nb_fit_mu);   # 0 if fit failed
> ##  3. print(m)q
> ##  4. print(v)
> ##  5. print(D)
> ##  6. print(deletion_propagation_coverage)
> ##     -1 if it was <1 after fitting (implying reference sequence is deleted)
> ##
> 
> plot_poisson = 0;
> pdf_output = 1;
> 
> this.print.level = 0
> #this.print.level = 2
> 
> for (e in commandArgs(TRUE)) {
+   ta = strsplit(e,"=",fixed=TRUE)[[1]]
+   if(length(ta)>1) {
+     temp = ta[2]
+     assign(ta[1],temp)
+     cat("assigned ",ta[1]," the value of |",temp,"|\n")
+   } else {
+     assign(ta[[1]][1],TRUE)
+     cat("assigned ",ta[1]," the value of TRUE\n")
+   }
+ }
assigned  distribution_file  the value of | 3a+_BHI_c50_out/07_error_calibration/96.unique_only_coverage_distribution.tab |
assigned  plot_file  the value of | 3a+_BHI_c50_out/output/calibration/96.unique_coverage.pdf |
assigned  deletion_propagation_pr_cutoff  the value of | 0.00313728 |
> 
> deletion_propagation_pr_cutoff = as.numeric(deletion_propagation_pr_cutoff);
> 
> ## initialize values to be filled in
> nb_fit_mu = 0
> nb_fit_size = 0
> m = 0
> v = 0
> D = 0
> deletion_propagation_coverage = -1
> 
> min_fraction_included_in_nb_fit = 0.01
> 
> #load data
> X<-read.table(distribution_file, header=T)
> 
> #table might be empty
> if (nrow(X) == 0)
+ {
+   #print out statistics
+   
+   print(nb_fit_size);
+   print(nb_fit_mu);
+   
+   print(m)
+   print(v)
+   print(D)
+   
+   print(deletion_propagation_coverage)
+   
+   q()
+ }
> 
> #create the distribution vector and fit
> Y<-rep(X$coverage, X$n)
> m<-mean(Y)
> v<-var(Y)
> D<-v/m
> 
> ###
> ## Smooth the distribution with a moving average window of size 5
> ## so that we can more reliably find it's maximum value
> ###
> 
> ma5 = c(1, 1, 1, 1, 1)/5;
> 
> ## filtering fails if there are too few points
> if (nrow(X) >= 5) {
+   X$ma = filter(X$n, ma5)
+ } else {
+ 	X$ma = X$n
+ }
> 
> i<-0
> max_n <- 0;
> min_i <- max( trunc(m/4), 1 ); #prevents zero for pathological distributions
> max_i <- i;
> for (i in min_i:length(X$ma))
+ {		
+   #cat(i, "\n")
+ 	if (!is.na(X$ma[i]) && (X$ma[i] > max_n))
+ 	{
+ 		max_n = X$ma[i];
+ 		max_i = i;
+ 	}
+ }
> 
> ##
> # Censor data on the right and left of the maximum
> ##
> 
> start_i = max(floor(max_i*0.5), 1);
> end_i = min(ceiling(max_i*1.5), length(X$ma));
> 
> if (start_i == end_i)
+ {
+   print(nb_fit_size);
+   print(nb_fit_mu);
+   
+   print(m)
+   print(v)
+   print(D)
+   
+   print(deletion_propagation_coverage)
+   
+   q()
+ }
> 
> cat("Fitting from coverage of ", start_i, " to ", end_i, ".\n", sep="")
Fitting from coverage of 2 to 6.
> 
> ##
> # Coarse grain so that we are only fitting a number of bins that is 1000-2000
> #
> # The later adjustment for doing the fits this way is to multiply the means
> # of the negative binomial and poisson distributions by the binning number.
> # (The size parameter of the negative binomial doesn't need to be adjusted.)
> ##
> 
> 
> num_per_bin = trunc((end_i - start_i) / 1000)
> 
> if (num_per_bin > 1) 
+ {
+   cat("Coarse-graining for fits\n")
+   start_i_for_fits = trunc(start_i/num_per_bin)
+   end_i_for_fits = ceiling(end_i/num_per_bin)
+   num_bins = end_i - start_i  + 1
+   cat("Fitting from coverage in adjusted bins ", start_i_for_fits, " to ", end_i_for_fits, ".\n", sep="")
+   cat("Number of bins ", num_bins, ". Each bin has ", num_per_bin, " coverage values.\n", sep="")
+ 
+   # Create a new vector where we've added together values in bins
+   X.for.fits = vector("double", end_i_for_fits)
+   for (i in start_i_for_fits:end_i_for_fits)
+   {
+     for (j in 1:num_per_bin)
+     {
+       if (i*num_per_bin+j <= length(X$n))
+       {
+         X.for.fits[i] = X.for.fits[i] + X$n[i*num_per_bin+j]
+       }
+     }
+   }
+ 
+ } else {
+   ## AVOID num_per_bin equalling zero!!
+   X.for.fits = X$n[1:end_i]
+   num_per_bin = 1
+   start_i_for_fits = start_i
+   end_i_for_fits = end_i
+ }
> 
> 
> ##
> # Now perform negative binomial fitting to the censored data
> ##
> 
> inner_total<-0;
> for (i in start_i_for_fits:end_i_for_fits)
+ {
+ 	inner_total = inner_total + X.for.fits[i]; 
+ }
> # Yes: it's correct to use X here because we want the overall total total
> total_total<-sum(X$n);
> 
> ## let's preconstruct these for speed
> dist = vector("double", end_i_for_fits)
> 
> f_nb <- function(par) {
+ 
+ 	mu = par[1];
+ 	size = par[2];
+ 
+   if ((mu <= 0) || (size <= 0))
+   {
+     return(0);
+   }
+   
+   cat(start_i_for_fits, " ", end_i_for_fits, "\n");
+   cat(mu, " ", size, "\n");
+   
+ 	dist<-c()
+ 	total <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{	
+ 		dist[i] <- dnbinom(i, size=size, mu=mu);
+ 		total <- total + dist[i] 
+ 	}
+ 	#print (mu, size)
+ 
+  	l <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{
+ 		l <- l + ((X.for.fits[i]/inner_total)-(dist[i]/total))^2;
+ 	}
+ 	return(l);
+ }
> 
> 
> 
> ## Fit negative binomial 
> ## - allow fit to fail and set all params to zero/empty if that is the case
> nb_fit = NULL
> ## as.numeric prevents overflow in sums involving integers
> mean_estimate = sum((as.numeric(1:end_i_for_fits)*as.numeric(X.for.fits)))/sum(as.numeric(X.for.fits))
> 
> nb_fit_mu = -1
> nb_fit_size = -1
> try_size = 100000
> try_means_index = 1
> #This is a list of different means to test <-  sometimes the actual mean doesn't lead to a fit
> try_means = c(mean_estimate, 
+               end_i_for_fits, 
+               start_i_for_fits, 
+               1*(end_i_for_fits + start_i_for_fits)/4,
+               2*(end_i_for_fits + start_i_for_fits)/4,
+               3*(end_i_for_fits + start_i_for_fits)/4
+               )
>               
>               
> nb_fit = c()
> 
> while ( ((nb_fit_mu < 0) || (nb_fit_size < 0) || (nb_fit$code != 1)) && (try_size > 0.001) && (try_means_index <= length(try_means)))
+ {
+   try_size = try_size / 10
+   try_mean = try_means[try_means_index]
+ 
+   ## SIZE ESTIMATE from the censored data can be negative, so try various values instead
+   cat("Try Mean: ", try_mean, " Size: ", try_size, "\n")
+ 
+   try( suppressWarnings(nb_fit<-nlm(f_nb, c(try_mean, try_size), iterlim=1000, print.level=this.print.level)) )
+ 
+   nb_fit_mu = nb_fit$estimate[1];
+   nb_fit_size = nb_fit$estimate[2];
+ 
+   cat("Fit Mean: ", nb_fit_mu, " Size: ", nb_fit_size, " Code: ", nb_fit$code, "\n")
+   
+   if (try_size <= 0.001) {
+     try_size = 100000
+     try_means_index = try_means_index + 1
+   }
+ }
Try Mean:  2.277778  Size:  10000 
2   6 
2.277778   10000 
2   6 
2.277778   10000 
2   6 
2.27778   10000 
2   6 
2.277778   10000.01 
2   6 
2.214424   10000 
2   6 
2.214426   10000 
2   6 
2.214424   10000.01 
2   6 
1.679853   10000 
2   6 
1.679855   10000 
2   6 
1.679853   10000.01 
2   6 
1.830791   10000 
2   6 
1.830793   10000 
2   6 
1.830791   10000.01 
2   6 
1.805682   10000 
2   6 
1.805683   10000 
2   6 
1.805682   10000.01 
2   6 
1.804097   10000 
2   6 
1.804099   10000 
2   6 
1.804097   10000.01 
2   6 
1.804117   10000 
2   6 
1.804119   10000 
2   6 
1.804117   10000.01 
2   6 
1.804117   10000 
2   6 
1.804298   10000 
2   6 
1.803937   10000 
2   6 
1.804117   10001 
2   6 
1.804117   9999 
2   6 
1.804118   10000 
2   6 
1.804299   10000 
2   6 
1.803938   10000 
2   6 
1.804118   10001 
2   6 
1.804118   9999 
Fit Mean:  1.804118  Size:  10000  Code:  2 
Try Mean:  2.277778  Size:  1000 
2   6 
2.277778   1000 
2   6 
2.277778   1000 
2   6 
2.27778   1000 
2   6 
2.277778   1000.001 
2   6 
2.214417   1000 
2   6 
2.214419   1000 
2   6 
2.214417   1000.001 
2   6 
1.677222   1000 
2   6 
1.677223   1000 
2   6 
1.677222   1000.001 
2   6 
1.829976   1000 
2   6 
1.829978   1000 
2   6 
1.829976   1000.001 
2   6 
1.804444   1000 
2   6 
1.804446   1000 
2   6 
1.804444   1000.001 
2   6 
1.802808   1000 
2   6 
1.80281   1000 
2   6 
1.802808   1000.001 
2   6 
1.802829   1000 
2   6 
1.802831   1000 
2   6 
1.802829   1000.001 
2   6 
1.802829   1000 
2   6 
1.802831   1000 
2   6 
1.802829   1000.001 
Fit Mean:  1.802829  Size:  1000  Code:  2 
Try Mean:  2.277778  Size:  100 
2   6 
2.277778   100 
2   6 
2.277778   100 
2   6 
2.27778   100 
2   6 
2.277778   100.0001 
2   6 
2.21437   100 
2   6 
2.214372   100 
2   6 
2.21437   100.0001 
2   6 
1.650493   100.0003 
2   6 
1.650495   100.0003 
2   6 
1.650493   100.0004 
2   6 
1.822278   100.0002 
2   6 
1.82228   100.0002 
2   6 
1.822278   100.0003 
2   6 
1.792254   100.0003 
2   6 
1.792256   100.0003 
2   6 
1.792254   100.0004 
2   6 
1.790014   100.0004 
2   6 
1.790016   100.0004 
2   6 
1.790014   100.0005 
2   6 
1.790049   100.0005 
2   6 
1.79005   100.0005 
2   6 
1.790049   100.0006 
2   6 
1.790143   100.0009 
2   6 
1.790145   100.0009 
2   6 
1.790143   100.001 
2   6 
1.790272   100.0022 
2   6 
1.790274   100.0022 
2   6 
1.790272   100.0023 
2   6 
1.790496   100.0062 
2   6 
1.790498   100.0062 
2   6 
1.790496   100.0063 
2   6 
1.79085   100.0171 
2   6 
1.790852   100.0171 
2   6 
1.79085   100.0172 
2   6 
1.79143   100.0468 
2   6 
1.791432   100.0468 
2   6 
1.79143   100.0469 
2   6 
1.792367   100.126 
2   6 
1.792369   100.126 
2   6 
1.792367   100.1261 
2   6 
1.793889   100.3356 
2   6 
1.79389   100.3356 
2   6 
1.793889   100.3357 
2   6 
1.796341   100.8837 
2   6 
1.796343   100.8837 
2   6 
1.796341   100.8838 
2   6 
1.800226   102.2941 
2   6 
1.800227   102.2941 
2   6 
1.800226   102.2942 
2   6 
1.806062   105.7985 
2   6 
1.806064   105.7985 
2   6 
1.806062   105.7987 
2   6 
1.813731   113.9136 
2   6 
1.813733   113.9136 
2   6 
1.813731   113.9137 
2   6 
1.821147   130.8138 
2   6 
1.821149   130.8138 
2   6 
1.821147   130.814 
2   6 
1.823568   162.6927 
2   6 
1.82357   162.6927 
2   6 
1.823568   162.6929 
2   6 
1.815832   215.7185 
2   6 
1.815834   215.7185 
2   6 
1.815832   215.7187 
2   6 
1.802939   284.2648 
2   6 
1.80294   284.2648 
2   6 
1.802939   284.2651 
2   6 
1.79512   355.5103 
2   6 
1.795122   355.5103 
2   6 
1.79512   355.5107 
2   6 
1.792373   443.5808 
2   6 
1.792375   443.5808 
2   6 
1.792373   443.5813 
2   6 
1.79416   584.2652 
2   6 
1.794161   584.2652 
2   6 
1.79416   584.2658 
2   6 
1.799679   786.892 
2   6 
1.799681   786.892 
2   6 
1.799679   786.8927 
2   6 
1.804786   1028.854 
2   6 
1.804788   1028.854 
2   6 
1.804786   1028.855 
2   6 
1.807466   1308.315 
2   6 
1.807468   1308.315 
2   6 
1.807466   1308.316 
2   6 
1.807946   1694.195 
2   6 
1.807948   1694.195 
2   6 
1.807946   1694.197 
2   6 
1.806403   2249.738 
2   6 
1.806405   2249.738 
2   6 
1.806403   2249.74 
2   6 
1.804145   2974.519 
2   6 
1.804146   2974.519 
2   6 
1.804145   2974.522 
2   6 
1.80262   3856.096 
2   6 
1.802622   3856.096 
2   6 
1.80262   3856.1 
2   6 
1.802162   5007.304 
2   6 
1.802164   5007.304 
2   6 
1.802162   5007.309 
2   6 
1.802691   6640.455 
2   6 
1.802693   6640.455 
2   6 
1.802691   6640.462 
2   6 
1.80375   8823.325 
2   6 
1.803752   8823.325 
2   6 
1.80375   8823.334 
2   6 
1.804651   11575.96 
2   6 
1.804653   11575.96 
2   6 
1.804651   11575.97 
2   6 
1.805074   15101.11 
2   6 
1.805075   15101.11 
2   6 
1.805074   15101.12 
2   6 
1.805   19915.78 
2   6 
1.805002   19915.78 
2   6 
1.805   19915.8 
2   6 
1.804582   26422.76 
2   6 
1.804584   26422.76 
2   6 
1.804582   26422.79 
2   6 
1.804135   34701.01 
2   6 
1.804137   34701.01 
2   6 
1.804135   34701.04 
2   6 
1.803859   45791.22 
2   6 
1.80386   45791.22 
2   6 
1.803859   45791.27 
2   6 
1.803851   59028.97 
2   6 
1.803853   59028.97 
2   6 
1.803851   59029.03 
2   6 
1.804028   79000.21 
2   6 
1.80403   79000.21 
2   6 
1.804028   79000.29 
2   6 
1.804299   114205.6 
2   6 
1.804301   114205.6 
2   6 
1.804299   114205.7 
2   6 
1.804381   139245.8 
2   6 
1.804383   139245.8 
2   6 
1.804381   139245.9 
2   6 
1.804427   164408 
2   6 
1.804428   164408 
2   6 
1.804427   164408.1 
2   6 
1.804404   222963.9 
2   6 
1.804405   222963.9 
2   6 
1.804404   222964.1 
2   6 
1.804332   227444.2 
2   6 
1.804334   227444.2 
2   6 
1.804332   227444.5 
2   6 
1.804291   253393 
2   6 
1.804293   253393 
2   6 
1.804291   253393.2 
2   6 
1.804276   251052 
2   6 
1.804289   253158.9 
2   6 
1.804291   253369.6 
2   6 
1.804291   253390.6 
2   6 
1.804291   253392.8 
2   6 
1.804471   253393 
2   6 
1.80411   253393 
2   6 
1.804291   253418.3 
2   6 
1.804291   253367.6 
2   6 
1.80426   269961.9 
2   6 
1.804441   269961.9 
2   6 
1.80408   269961.9 
2   6 
1.80426   269988.9 
2   6 
1.80426   269935 
2   6 
1.804161   369987.9 
2   6 
1.804341   369987.9 
2   6 
1.80398   369987.9 
2   6 
1.804161   370024.9 
2   6 
1.804161   369950.9 
2   6 
1.804148   470013.8 
2   6 
1.804328   470013.8 
2   6 
1.803967   470013.8 
2   6 
1.804148   470060.8 
2   6 
1.804148   469966.8 
2   6 
1.804173   570039.8 
2   6 
1.804354   570039.8 
2   6 
1.803993   570039.8 
2   6 
1.804173   570096.8 
2   6 
1.804173   569982.8 
2   6 
1.804205   670065.7 
2   6 
1.804385   670065.7 
2   6 
1.804024   670065.7 
2   6 
1.804205   670132.7 
2   6 
1.804205   669998.7 
2   6 
1.804234   770091.6 
2   6 
1.804414   770091.6 
2   6 
1.804053   770091.6 
2   6 
1.804234   770168.6 
2   6 
1.804234   770014.6 
Fit Mean:  1.804234  Size:  770091.6  Code:  5 
Try Mean:  2.277778  Size:  10 
2   6 
2.277778   10 
2   6 
2.277778   10 
2   6 
2.27778   10 
2   6 
2.277778   10.00001 
2   6 
2.215459   10.001 
2   6 
2.215461   10.001 
2   6 
2.215459   10.00101 
2   6 
1.332142   10.03227 
2   6 
1.332143   10.03227 
2   6 
1.332142   10.03228 
2   6 
1.799871   10.02156 
2   6 
1.799873   10.02156 
2   6 
1.799871   10.02157 
2   6 
1.699033   10.03149 
2   6 
1.699035   10.03149 
2   6 
1.699033   10.0315 
2   6 
1.66924   10.04229 
2   6 
1.669241   10.04229 
2   6 
1.66924   10.0423 
2   6 
1.67097   10.04978 
2   6 
1.670971   10.04978 
2   6 
1.67097   10.04979 
2   6 
1.689003   10.21277 
2   6 
1.689005   10.21277 
2   6 
1.689003   10.21278 
2   6 
1.709682   10.56448 
2   6 
1.709683   10.56448 
2   6 
1.709682   10.56449 
2   6 
1.743892   11.54839 
2   6 
1.743894   11.54839 
2   6 
1.743892   11.5484 
2   6 
1.777465   13.35034 
2   6 
1.777467   13.35034 
2   6 
1.777465   13.35035 
2   6 
1.799179   16.54295 
2   6 
1.799181   16.54295 
2   6 
1.799179   16.54297 
2   6 
1.792425   21.64724 
2   6 
1.792427   21.64724 
2   6 
1.792425   21.64726 
2   6 
1.769385   28.40687 
2   6 
1.769387   28.40687 
2   6 
1.769385   28.4069 
2   6 
1.761593   36.61466 
2   6 
1.761594   36.61466 
2   6 
1.761593   36.61469 
2   6 
1.77008   48.80079 
2   6 
1.770082   48.80079 
2   6 
1.77008   48.80084 
2   6 
1.786297   65.9486 
2   6 
1.786298   65.9486 
2   6 
1.786297   65.94866 
2   6 
1.797406   87.22604 
2   6 
1.797408   87.22604 
2   6 
1.797406   87.22613 
2   6 
1.80135   114.9663 
2   6 
1.801352   114.9663 
2   6 
1.80135   114.9664 
2   6 
1.800542   152.2109 
2   6 
1.800544   152.2109 
2   6 
1.800542   152.211 
2   6 
1.799041   201.3434 
2   6 
1.799043   201.3434 
2   6 
1.799041   201.3436 
2   6 
1.799109   266.5357 
2   6 
1.799111   266.5357 
2   6 
1.799109   266.5359 
2   6 
1.800527   353.7063 
2   6 
1.800529   353.7063 
2   6 
1.800527   353.7066 
2   6 
1.802115   469.2093 
2   6 
1.802116   469.2093 
2   6 
1.802115   469.2097 
2   6 
1.803095   621.7123 
2   6 
1.803097   621.7123 
2   6 
1.803095   621.713 
2   6 
1.803435   823.6262 
2   6 
1.803437   823.6262 
2   6 
1.803435   823.627 
2   6 
1.803475   1091.133 
2   6 
1.803477   1091.133 
2   6 
1.803475   1091.134 
2   6 
1.803518   1445.493 
2   6 
1.803519   1445.493 
2   6 
1.803518   1445.494 
2   6 
1.803655   1915.157 
2   6 
1.803657   1915.157 
2   6 
1.803655   1915.159 
2   6 
1.803836   2537.708 
2   6 
1.803837   2537.708 
2   6 
1.803836   2537.711 
2   6 
1.803984   3362.16 
2   6 
1.803986   3362.16 
2   6 
1.803984   3362.164 
2   6 
1.804072   4453.381 
2   6 
1.804074   4453.381 
2   6 
1.804072   4453.386 
2   6 
1.804114   5900.172 
2   6 
1.804116   5900.172 
2   6 
1.804114   5900.178 
2   6 
1.804138   7816.932 
2   6 
1.80414   7816.932 
2   6 
1.804138   7816.94 
2   6 
1.80416   10345.99 
2   6 
1.804162   10345.99 
2   6 
1.80416   10346 
2   6 
1.804185   13715.73 
2   6 
1.804186   13715.73 
2   6 
1.804185   13715.74 
2   6 
1.804206   18134.93 
2   6 
1.804208   18134.93 
2   6 
1.804206   18134.95 
2   6 
1.804222   24038.11 
2   6 
1.804223   24038.11 
2   6 
1.804222   24038.14 
2   6 
1.804232   31925.14 
2   6 
1.804234   31925.14 
2   6 
1.804232   31925.18 
2   6 
1.804239   42181.28 
2   6 
1.804241   42181.28 
2   6 
1.804239   42181.32 
2   6 
1.804242   52437.41 
2   6 
1.804244   52437.41 
2   6 
1.804242   52437.46 
2   6 
1.804244   62693.55 
2   6 
1.804246   62693.55 
2   6 
1.804244   62693.61 
2   6 
1.804246   72949.68 
2   6 
1.804248   72949.68 
2   6 
1.804246   72949.75 
2   6 
1.804247   83205.81 
2   6 
1.804249   83205.81 
2   6 
1.804247   83205.9 
Fit Mean:  1.804247  Size:  83205.81  Code:  5 
Try Mean:  2.277778  Size:  1 
2   6 
2.277778   1 
2   6 
2.277778   1 
2   6 
2.27778   1 
2   6 
2.277778   1.000001 
2   6 
2.241103   1.017549 
2   6 
2.241106   1.017549 
2   6 
2.241103   1.01755 
2   6 
2.20373   1.035574 
2   6 
2.203733   1.035574 
2   6 
2.20373   1.035575 
2   6 
2.165679   1.054042 
2   6 
2.165681   1.054042 
2   6 
2.165679   1.054043 
2   6 
2.126977   1.072915 
2   6 
2.126979   1.072915 
2   6 
2.126977   1.072916 
2   6 
2.087663   1.092151 
2   6 
2.087666   1.092151 
2   6 
2.087663   1.092152 
2   6 
2.047788   1.111703 
2   6 
2.04779   1.111703 
2   6 
2.047788   1.111704 
2   6 
2.007412   1.131518 
2   6 
2.007414   1.131518 
2   6 
2.007412   1.131519 
2   6 
1.966614   1.151537 
2   6 
1.966616   1.151537 
2   6 
1.966614   1.151539 
2   6 
1.925486   1.171698 
2   6 
1.925488   1.171698 
2   6 
1.925486   1.171699 
2   6 
1.884135   1.191931 
2   6 
1.884137   1.191931 
2   6 
1.884135   1.191933 
2   6 
1.842689   1.212163 
2   6 
1.842691   1.212163 
2   6 
1.842689   1.212164 
Fit Mean:  -22.33339  Size:  12.98081  Code:  1 
Try Mean:  2.277778  Size:  0.1 
2   6 
2.277778   0.1 
2   6 
2.277778   0.1 
2   6 
2.27778   0.1 
2   6 
2.277778   0.100001 
2   6 
2.271619   0.1366972 
2   6 
2.271621   0.1366972 
2   6 
2.271619   0.1366982 
Fit Mean:  -12.44593  Size:  34.06242  Code:  1 
Try Mean:  2.277778  Size:  0.01 
2   6 
2.277778   0.01 
2   6 
2.277778   0.01 
2   6 
2.27778   0.01 
2   6 
2.277778   0.010001 
2   6 
2.277121   0.05013722 
2   6 
2.277123   0.05013722 
2   6 
2.277121   0.05013822 
2   6 
0.3870878   4.155659 
2   6 
1.722881   1.254055 
2   6 
1.722882   1.254055 
2   6 
1.722881   1.254056 
Fit Mean:  -71.24529  Size:  147.1943  Code:  1 
Try Mean:  2.277778  Size:  0.001 
2   6 
2.277778   0.001 
2   6 
2.277778   0.001 
2   6 
2.27778   0.001 
2   6 
2.277778   0.001001 
2   6 
2.277712   0.04150664 
2   6 
2.277714   0.04150664 
2   6 
2.277712   0.04150764 
2   6 
0.6379904   3.643315 
2   6 
1.525714   1.693342 
2   6 
1.525716   1.693342 
2   6 
1.525714   1.693344 
Fit Mean:  -0.7245208  Size:  5.475545  Code:  1 
Try Mean:  6  Size:  10000 
2   6 
6   10000 
2   6 
6   10000 
2   6 
6.000006   10000 
2   6 
6   10000.01 
2   6 
5.906035   10000 
2   6 
5.906041   10000 
2   6 
5.906035   10000.01 
2   6 
5.810505   10000 
2   6 
5.810511   10000 
2   6 
5.810505   10000.01 
2   6 
5.713371   10000 
2   6 
5.713377   10000 
2   6 
5.713371   10000.01 
2   6 
5.614593   10000 
2   6 
5.614599   10000 
2   6 
5.614593   10000.01 
2   6 
5.514134   10000 
2   6 
5.514139   10000 
2   6 
5.514134   10000.01 
2   6 
5.411959   10000 
2   6 
5.411965   10000 
2   6 
5.411959   10000.01 
2   6 
5.30804   10000 
2   6 
5.308045   10000 
2   6 
5.30804   10000.01 
2   6 
5.20235   10000 
2   6 
5.202356   10000 
2   6 
5.20235   10000.01 
2   6 
5.094875   10000 
2   6 
5.09488   10000 
2   6 
5.094875   10000.01 
2   6 
4.985604   10000 
2   6 
4.985609   10000 
2   6 
4.985604   10000.01 
2   6 
4.874543   10000 
2   6 
4.874548   10000 
2   6 
4.874543   10000.01 
2   6 
4.76171   10000 
2   6 
4.761715   10000 
2   6 
4.76171   10000.01 
2   6 
4.647141   10000 
2   6 
4.647145   10000 
2   6 
4.647141   10000.01 
2   6 
4.530893   10000 
2   6 
4.530898   10000 
2   6 
4.530893   10000.01 
2   6 
4.413053   10000 
2   6 
4.413057   10000 
2   6 
4.413053   10000.01 
2   6 
4.293734   10000 
2   6 
4.293739   10000 
2   6 
4.293734   10000.01 
2   6 
4.173091   10000 
2   6 
4.173095   10000 
2   6 
4.173091   10000.01 
2   6 
4.051319   10000 
2   6 
4.051323   10000 
2   6 
4.051319   10000.01 
2   6 
3.928664   10000 
2   6 
3.928668   10000 
2   6 
3.928664   10000.01 
2   6 
3.805425   10000 
2   6 
3.805429   10000 
2   6 
3.805425   10000.01 
2   6 
3.681966   10000 
2   6 
3.68197   10000 
2   6 
3.681966   10000.01 
Fit Mean:  -70.52772  Size:  10000  Code:  1 
Try Mean:  6  Size:  1000 
2   6 
6   1000 
2   6 
6   1000 
2   6 
6.000006   1000 
2   6 
6   1000.001 
2   6 
5.90645   1000 
2   6 
5.906456   1000 
2   6 
5.90645   1000.001 
2   6 
5.811348   1000 
2   6 
5.811354   1000 
2   6 
5.811348   1000.001 
2   6 
5.714654   1000 
2   6 
5.71466   1000 
2   6 
5.714654   1000.001 
2   6 
5.616329   1000 
2   6 
5.616334   1000 
2   6 
5.616329   1000.001 
2   6 
5.516335   1000 
2   6 
5.516341   1000 
2   6 
5.516335   1000.001 
2   6 
5.414639   1000 
2   6 
5.414645   1000 
2   6 
5.414639   1000.001 
2   6 
5.311211   1000 
2   6 
5.311217   1000 
2   6 
5.311211   1000.001 
2   6 
5.206027   1000 
2   6 
5.206032   1000 
2   6 
5.206027   1000.001 
2   6 
5.099069   1000 
2   6 
5.099074   1000 
2   6 
5.099069   1000.001 
2   6 
4.990328   1000 
2   6 
4.990333   1000 
2   6 
4.990328   1000.001 
2   6 
4.879808   1000 
2   6 
4.879813   1000 
2   6 
4.879808   1000.001 
2   6 
4.767524   1000 
2   6 
4.767529   1000 
2   6 
4.767524   1000.001 
2   6 
4.653513   1000 
2   6 
4.653517   1000 
2   6 
4.653513   1000.001 
2   6 
4.537828   1000 
2   6 
4.537832   1000 
2   6 
4.537828   1000.001 
2   6 
4.420551   1000 
2   6 
4.420555   1000 
2   6 
4.420551   1000.001 
2   6 
4.301793   1000 
2   6 
4.301797   1000 
2   6 
4.301793   1000.001 
2   6 
4.181702   1000 
2   6 
4.181706   1000 
2   6 
4.181702   1000.001 
2   6 
4.060468   1000 
2   6 
4.060472   1000 
2   6 
4.060468   1000.001 
2   6 
3.938328   1000 
2   6 
3.938332   1000 
2   6 
3.938328   1000.001 
2   6 
3.815573   1000 
2   6 
3.815577   1000 
2   6 
3.815573   1000.001 
2   6 
3.692555   1000 
2   6 
3.692558   1000 
2   6 
3.692555   1000.001 
Fit Mean:  -96.08469  Size:  999.9998  Code:  1 
Try Mean:  6  Size:  100 
2   6 
6   100 
2   6 
6   100 
2   6 
6.000006   100 
2   6 
6   100.0001 
2   6 
5.910444   99.99987 
2   6 
5.91045   99.99987 
2   6 
5.910444   99.99997 
2   6 
5.819451   99.99974 
2   6 
5.819457   99.99974 
2   6 
5.819451   99.99984 
2   6 
5.726983   99.99962 
2   6 
5.726989   99.99962 
2   6 
5.726983   99.99972 
2   6 
5.633005   99.9995 
2   6 
5.633011   99.9995 
2   6 
5.633005   99.9996 
2   6 
5.537483   99.99938 
2   6 
5.537488   99.99938 
2   6 
5.537483   99.99948 
2   6 
5.440383   99.99927 
2   6 
5.440388   99.99927 
2   6 
5.440383   99.99937 
2   6 
5.341676   99.99916 
2   6 
5.341682   99.99916 
2   6 
5.341676   99.99926 
2   6 
5.241338   99.99906 
2   6 
5.241343   99.99906 
2   6 
5.241338   99.99916 
2   6 
5.139347   99.99896 
2   6 
5.139352   99.99896 
2   6 
5.139347   99.99906 
2   6 
5.03569   99.99887 
2   6 
5.035695   99.99887 
2   6 
5.03569   99.99897 
2   6 
4.930361   99.99878 
2   6 
4.930366   99.99878 
2   6 
4.930361   99.99888 
2   6 
4.823365   99.9987 
2   6 
4.82337   99.9987 
2   6 
4.823365   99.9988 
2   6 
4.714719   99.99862 
2   6 
4.714724   99.99862 
2   6 
4.714719   99.99872 
2   6 
4.604458   99.99855 
2   6 
4.604463   99.99855 
2   6 
4.604458   99.99865 
2   6 
4.492633   99.99848 
2   6 
4.492637   99.99848 
2   6 
4.492633   99.99858 
2   6 
4.379319   99.99842 
2   6 
4.379323   99.99842 
2   6 
4.379319   99.99852 
2   6 
4.264618   99.99836 
2   6 
4.264622   99.99836 
2   6 
4.264618   99.99846 
2   6 
4.148665   99.99831 
2   6 
4.148669   99.99831 
2   6 
4.148665   99.99841 
2   6 
4.031629   99.99827 
2   6 
4.031633   99.99827 
2   6 
4.031629   99.99837 
2   6 
3.913724   99.99823 
2   6 
3.913728   99.99823 
2   6 
3.913724   99.99833 
2   6 
3.795211   99.99819 
2   6 
3.795214   99.99819 
2   6 
3.795211   99.99829 
2   6 
3.676401   99.99816 
2   6 
3.676405   99.99816 
2   6 
3.676401   99.99826 
Fit Mean:  -188.5024  Size:  99.96072  Code:  1 
Try Mean:  6  Size:  10 
2   6 
6   10 
2   6 
6   10 
2   6 
6.000006   10 
2   6 
6   10.00001 
2   6 
5.938789   9.992602 
2   6 
5.938795   9.992602 
2   6 
5.938789   9.992612 
2   6 
5.876869   9.985338 
2   6 
5.876874   9.985338 
2   6 
5.876869   9.985348 
2   6 
5.814222   9.978211 
2   6 
5.814228   9.978211 
2   6 
5.814222   9.978221 
2   6 
5.750834   9.971224 
2   6 
5.75084   9.971224 
2   6 
5.750834   9.971234 
2   6 
5.686687   9.964381 
2   6 
5.686693   9.964381 
2   6 
5.686687   9.964391 
2   6 
5.621764   9.957687 
2   6 
5.62177   9.957687 
2   6 
5.621764   9.957697 
2   6 
5.556048   9.951143 
2   6 
5.556054   9.951143 
2   6 
5.556048   9.951153 
2   6 
5.489523   9.944755 
2   6 
5.489528   9.944755 
2   6 
5.489523   9.944765 
2   6 
5.42217   9.938526 
2   6 
5.422175   9.938526 
2   6 
5.42217   9.938536 
2   6 
5.353972   9.932461 
2   6 
5.353978   9.932461 
2   6 
5.353972   9.932471 
2   6 
5.284914   9.926562 
2   6 
5.284919   9.926562 
2   6 
5.284914   9.926572 
2   6 
5.214977   9.920835 
2   6 
5.214982   9.920835 
2   6 
5.214977   9.920845 
2   6 
5.144145   9.915284 
2   6 
5.144151   9.915284 
2   6 
5.144145   9.915294 
2   6 
5.072404   9.909912 
2   6 
5.072409   9.909912 
2   6 
5.072404   9.909922 
2   6 
4.999737   9.904725 
2   6 
4.999742   9.904725 
2   6 
4.999737   9.904735 
2   6 
4.92613   9.899726 
2   6 
4.926135   9.899726 
2   6 
4.92613   9.899735 
2   6 
4.851572   9.894919 
2   6 
4.851577   9.894919 
2   6 
4.851572   9.894929 
2   6 
4.77605   9.890309 
2   6 
4.776055   9.890309 
2   6 
4.77605   9.890319 
2   6 
4.699556   9.885901 
2   6 
4.699561   9.885901 
2   6 
4.699556   9.885911 
2   6 
4.622083   9.881698 
2   6 
4.622088   9.881698 
2   6 
4.622083   9.881708 
2   6 
4.543628   9.877704 
2   6 
4.543633   9.877704 
2   6 
4.543628   9.877714 
2   6 
4.46419   9.873924 
2   6 
4.464194   9.873924 
2   6 
4.46419   9.873934 
2   6 
4.383772   9.870362 
2   6 
4.383776   9.870362 
2   6 
4.383772   9.870371 
2   6 
4.302385   9.867019 
2   6 
4.302389   9.867019 
2   6 
4.302385   9.867029 
2   6 
4.220042   9.863901 
2   6 
4.220046   9.863901 
2   6 
4.220042   9.863911 
2   6 
4.136766   9.86101 
2   6 
4.13677   9.86101 
2   6 
4.136766   9.86102 
2   6 
4.052587   9.858348 
2   6 
4.052591   9.858348 
2   6 
4.052587   9.858358 
2   6 
3.967544   9.855918 
2   6 
3.967548   9.855918 
2   6 
3.967544   9.855928 
2   6 
3.881686   9.85372 
2   6 
3.88169   9.85372 
2   6 
3.881686   9.85373 
2   6 
3.795078   9.851756 
2   6 
3.795081   9.851756 
2   6 
3.795078   9.851766 
2   6 
3.707794   9.850025 
2   6 
3.707798   9.850025 
2   6 
3.707794   9.850035 
2   6 
3.619929   9.848526 
2   6 
3.619933   9.848526 
2   6 
3.619929   9.848536 
2   6 
3.531593   9.847257 
2   6 
3.531596   9.847257 
2   6 
3.531593   9.847267 
2   6 
3.442916   9.846215 
2   6 
3.44292   9.846215 
2   6 
3.442916   9.846225 
2   6 
3.354052   9.845395 
2   6 
3.354055   9.845395 
2   6 
3.354052   9.845405 
2   6 
3.265177   9.84479 
2   6 
3.26518   9.84479 
2   6 
3.265177   9.8448 
Fit Mean:  -37.96664  Size:  9.660469  Code:  1 
Try Mean:  6  Size:  1 
2   6 
6   1 
2   6 
6   1 
2   6 
6.000006   1 
2   6 
6   1.000001 
2   6 
5.988831   0.9527342 
2   6 
5.988837   0.9527342 
2   6 
5.988831   0.9527352 
2   6 
5.978175   0.9050799 
2   6 
5.978181   0.9050799 
2   6 
5.978175   0.9050809 
2   6 
5.968043   0.8570339 
2   6 
5.968049   0.8570339 
2   6 
5.968043   0.8570349 
2   6 
5.958443   0.8085945 
2   6 
5.958449   0.8085945 
2   6 
5.958443   0.8085955 
2   6 
5.949387   0.7597618 
2   6 
5.949393   0.7597618 
2   6 
5.949387   0.7597628 
2   6 
5.940883   0.7105376 
2   6 
5.940889   0.7105376 
2   6 
5.940883   0.7105386 
2   6 
5.93294   0.660926 
2   6 
5.932946   0.660926 
2   6 
5.93294   0.660927 
2   6 
5.925566   0.6109342 
2   6 
5.925571   0.6109342 
2   6 
5.925566   0.6109352 
2   6 
5.918767   0.5605722 
2   6 
5.918773   0.5605722 
2   6 
5.918767   0.5605732 
2   6 
5.91255   0.5098541 
2   6 
5.912556   0.5098541 
2   6 
5.91255   0.5098551 
2   6 
5.90692   0.4587985 
2   6 
5.906926   0.4587985 
2   6 
5.90692   0.4587995 
2   6 
5.90188   0.407429 
2   6 
5.901886   0.407429 
2   6 
5.90188   0.40743 
2   6 
5.897432   0.3557753 
2   6 
5.897438   0.3557753 
2   6 
5.897432   0.3557763 
2   6 
5.893576   0.3038741 
2   6 
5.893582   0.3038741 
2   6 
5.893576   0.3038751 
2   6 
5.890311   0.2517699 
2   6 
5.890317   0.2517699 
2   6 
5.890311   0.2517709 
2   6 
5.887632   0.1995161 
2   6 
5.887638   0.1995161 
2   6 
5.887632   0.1995171 
2   6 
5.885533   0.1471765 
2   6 
5.885539   0.1471765 
2   6 
5.885533   0.1471775 
Fit Mean:  -5.615381  Size:  -349.0744  Code:  1 
Try Mean:  6  Size:  0.1 
2   6 
6   0.1 
2   6 
6   0.1 
2   6 
6.000006   0.1 
2   6 
6   0.100001 
2   6 
5.999013   0.04651287 
2   6 
5.999019   0.04651287 
2   6 
5.999013   0.04651387 
Fit Mean:  36.30611  Size:  -91.61843  Code:  1 
Try Mean:  6  Size:  0.01 
2   6 
6   0.01 
2   6 
6   0.01 
2   6 
6.000006   0.01 
2   6 
6   0.010001 
Fit Mean:  5.999904  Size:  -0.04307788  Code:  1 
Try Mean:  6  Size:  0.001 
2   6 
6   0.001 
2   6 
6   0.001 
2   6 
6.000006   0.001 
2   6 
6   0.001001 
Fit Mean:  5.99999  Size:  -0.05201578  Code:  1 
Try Mean:  2  Size:  10000 
2   6 
2   10000 
2   6 
2   10000 
2   6 
2.000002   10000 
2   6 
2   10000.01 
2   6 
1.969934   10000 
2   6 
1.969936   10000 
2   6 
1.969934   10000.01 
2   6 
1.78662   10000 
2   6 
1.786622   10000 
2   6 
1.78662   10000.01 
2   6 
1.805583   10000 
2   6 
1.805585   10000 
2   6 
1.805583   10000.01 
2   6 
1.80413   10000 
2   6 
1.804131   10000 
2   6 
1.80413   10000.01 
2   6 
1.804117   10000 
2   6 
1.804119   10000 
2   6 
1.804117   10000.01 
2   6 
1.804117   10000 
2   6 
1.804119   10000 
2   6 
1.804117   10000.01 
Fit Mean:  1.804117  Size:  10000  Code:  2 
Try Mean:  2  Size:  1000 
2   6 
2   1000 
2   6 
2   1000 
2   6 
2.000002   1000 
2   6 
2   1000.001 
2   6 
1.969801   1000 
2   6 
1.969803   1000 
2   6 
1.969801   1000.001 
2   6 
1.785013   1000 
2   6 
1.785015   1000 
2   6 
1.785013   1000.001 
2   6 
1.804339   1000 
2   6 
1.804341   1000 
2   6 
1.804339   1000.001 
2   6 
1.802842   1000 
2   6 
1.802844   1000 
2   6 
1.802842   1000.001 
2   6 
1.802829   1000 
2   6 
1.802831   1000 
2   6 
1.802829   1000.001 
2   6 
1.802829   1000 
2   6 
1.802831   1000 
2   6 
1.802829   1000.001 
Fit Mean:  1.802829  Size:  1000  Code:  2 
Try Mean:  2  Size:  100 
2   6 
2   100 
2   6 
2   100 
2   6 
2.000002   100 
2   6 
2   100.0001 
2   6 
1.96852   100 
2   6 
1.968522   100 
2   6 
1.96852   100.0001 
2   6 
1.768838   100.0001 
2   6 
1.76884   100.0001 
2   6 
1.768838   100.0002 
2   6 
1.792048   100.0002 
2   6 
1.792049   100.0002 
2   6 
1.792048   100.0003 
2   6 
1.790071   100.0002 
2   6 
1.790073   100.0002 
2   6 
1.790071   100.0003 
2   6 
1.790049   100.0003 
2   6 
1.790051   100.0003 
2   6 
1.790049   100.0004 
2   6 
1.789941   100.0009 
2   6 
1.789943   100.0009 
2   6 
1.789941   100.001 
2   6 
1.78981   100.0023 
2   6 
1.789812   100.0023 
2   6 
1.78981   100.0024 
2   6 
1.789573   100.0068 
2   6 
1.789574   100.0068 
2   6 
1.789573   100.0069 
2   6 
1.789205   100.0189 
2   6 
1.789207   100.0189 
2   6 
1.789205   100.019 
2   6 
1.788603   100.0521 
2   6 
1.788605   100.0521 
2   6 
1.788603   100.0522 
2   6 
1.787642   100.14 
2   6 
1.787644   100.14 
2   6 
1.787642   100.1401 
2   6 
1.786108   100.373 
2   6 
1.78611   100.373 
2   6 
1.786108   100.3731 
2   6 
1.783705   100.9818 
2   6 
1.783707   100.9818 
2   6 
1.783705   100.9819 
2   6 
1.780072   102.5501 
2   6 
1.780074   102.5501 
2   6 
1.780072   102.5502 
2   6 
1.775029   106.4676 
2   6 
1.775031   106.4676 
2   6 
1.775029   106.4677 
2   6 
1.769293   115.7435 
2   6 
1.769295   115.7435 
2   6 
1.769293   115.7436 
2   6 
1.765718   136.2453 
2   6 
1.76572   136.2453 
2   6 
1.765718   136.2454 
2   6 
1.770127   178.252 
2   6 
1.770128   178.252 
2   6 
1.770127   178.2522 
2   6 
1.786349   248.9819 
2   6 
1.786351   248.9819 
2   6 
1.786349   248.9822 
2   6 
1.802818   326.8061 
2   6 
1.80282   326.8061 
2   6 
1.802818   326.8064 
2   6 
1.810481   389.2976 
2   6 
1.810483   389.2976 
2   6 
1.810481   389.298 
2   6 
1.814024   467.67 
2   6 
1.814025   467.67 
2   6 
1.814024   467.6704 
2   6 
1.813558   603.6679 
2   6 
1.813559   603.6679 
2   6 
1.813558   603.6685 
2   6 
1.80827   806.5057 
2   6 
1.808272   806.5057 
2   6 
1.80827   806.5065 
2   6 
1.802535   1050.914 
2   6 
1.802537   1050.914 
2   6 
1.802535   1050.915 
2   6 
1.799534   1318.79 
2   6 
1.799536   1318.79 
2   6 
1.799534   1318.791 
2   6 
1.798857   1687.411 
2   6 
1.798859   1687.411 
2   6 
1.798857   1687.412 
2   6 
1.800484   2247.083 
2   6 
1.800486   2247.083 
2   6 
1.800484   2247.085 
2   6 
1.803261   2996.623 
2   6 
1.803263   2996.623 
2   6 
1.803261   2996.626 
2   6 
1.805325   3876.819 
2   6 
1.805327   3876.819 
2   6 
1.805325   3876.823 
2   6 
1.806219   4974.651 
2   6 
1.806221   4974.651 
2   6 
1.806219   4974.656 
2   6 
1.806003   6531.058 
2   6 
1.806005   6531.058 
2   6 
1.806003   6531.065 
2   6 
1.804966   8695.073 
2   6 
1.804968   8695.073 
2   6 
1.804966   8695.082 
2   6 
1.803899   11433.24 
2   6 
1.803901   11433.24 
2   6 
1.803899   11433.25 
2   6 
1.803317   14805.61 
2   6 
1.803319   14805.61 
2   6 
1.803317   14805.63 
2   6 
1.803263   19476.22 
2   6 
1.803265   19476.22 
2   6 
1.803263   19476.24 
2   6 
1.803663   25627.02 
2   6 
1.803665   25627.02 
2   6 
1.803663   25627.05 
2   6 
1.804224   34195.1 
2   6 
1.804226   34195.1 
2   6 
1.804224   34195.13 
2   6 
1.80459   44423.38 
2   6 
1.804592   44423.38 
2   6 
1.80459   44423.42 
2   6 
1.804707   58180.97 
2   6 
1.804709   58180.97 
2   6 
1.804707   58181.03 
2   6 
1.804577   75125.79 
2   6 
1.804579   75125.79 
2   6 
1.804577   75125.86 
2   6 
1.804292   107546.9 
2   6 
1.804294   107546.9 
2   6 
1.804292   107547 
2   6 
1.804092   146248.1 
2   6 
1.804094   146248.1 
2   6 
1.804092   146248.2 
2   6 
1.804081   176064.2 
2   6 
1.804083   176064.2 
2   6 
1.804081   176064.3 
2   6 
1.804117   276084.2 
2   6 
1.804119   276084.2 
2   6 
1.804117   276084.4 
2   6 
1.804156   376104.2 
2   6 
1.804158   376104.2 
2   6 
1.804156   376104.5 
2   6 
1.804205   387863.5 
2   6 
1.804207   387863.5 
2   6 
1.804205   387863.9 
2   6 
1.804245   487883.5 
2   6 
1.804246   487883.5 
2   6 
1.804245   487884 
2   6 
1.804271   587903.5 
2   6 
1.804273   587903.5 
2   6 
1.804271   587904.1 
2   6 
1.804284   687923.5 
2   6 
1.804286   687923.5 
2   6 
1.804284   687924.2 
2   6 
1.804297   787943.5 
2   6 
1.804299   787943.5 
2   6 
1.804297   787944.3 
2   6 
1.804304   887963.5 
2   6 
1.804306   887963.5 
2   6 
1.804304   887964.4 
Fit Mean:  1.804304  Size:  887963.5  Code:  5 
Try Mean:  2  Size:  10 
2   6 
2   10 
2   6 
2   10 
2   6 
2.000002   10 
2   6 
2   10.00001 
2   6 
1.95954   10.00094 
2   6 
1.959542   10.00094 
2   6 
1.95954   10.00095 
2   6 
1.593385   10.01576 
2   6 
1.593387   10.01576 
2   6 
1.593385   10.01577 
2   6 
1.687681   10.01698 
2   6 
1.687683   10.01698 
2   6 
1.687681   10.01699 
2   6 
1.672673   10.02198 
2   6 
1.672674   10.02198 
2   6 
1.672673   10.02199 
2   6 
1.670747   10.02823 
2   6 
1.670748   10.02823 
2   6 
1.670747   10.02824 
2   6 
1.662238   10.08899 
2   6 
1.662239   10.08899 
2   6 
1.662238   10.089 
2   6 
1.653128   10.23085 
2   6 
1.65313   10.23085 
2   6 
1.653128   10.23086 
2   6 
1.640388   10.66717 
2   6 
1.640389   10.66717 
2   6 
1.640388   10.66718 
2   6 
1.630218   11.74733 
2   6 
1.63022   11.74733 
2   6 
1.630218   11.74734 
2   6 
1.634864   14.5125 
2   6 
1.634866   14.5125 
2   6 
1.634864   14.51251 
2   6 
1.683159   21.35296 
2   6 
1.683161   21.35296 
2   6 
1.683159   21.35298 
2   6 
1.769311   32.22381 
2   6 
1.769313   32.22381 
2   6 
1.769311   32.22384 
2   6 
1.801903   38.14649 
2   6 
1.801905   38.14649 
2   6 
1.801903   38.14653 
2   6 
1.815167   43.49384 
2   6 
1.815169   43.49384 
2   6 
1.815167   43.49388 
2   6 
1.821928   55.08835 
2   6 
1.82193   55.08835 
2   6 
1.821928   55.0884 
2   6 
1.811512   72.29179 
2   6 
1.811514   72.29179 
2   6 
1.811512   72.29186 
2   6 
1.794356   94.804 
2   6 
1.794358   94.804 
2   6 
1.794356   94.80409 
2   6 
1.785664   120.2574 
2   6 
1.785666   120.2574 
2   6 
1.785664   120.2575 
2   6 
1.785264   155.9408 
2   6 
1.785266   155.9408 
2   6 
1.785264   155.941 
2   6 
1.792122   209.51 
2   6 
1.792124   209.51 
2   6 
1.792122   209.5102 
2   6 
1.800741   279.1556 
2   6 
1.800743   279.1556 
2   6 
1.800741   279.1559 
2   6 
1.80614   362.8756 
2   6 
1.806142   362.8756 
2   6 
1.80614   362.8759 
2   6 
1.807775   472.9947 
2   6 
1.807777   472.9947 
2   6 
1.807775   472.9952 
2   6 
1.806261   625.8216 
2   6 
1.806263   625.8216 
2   6 
1.806261   625.8222 
2   6 
1.803561   827.7686 
2   6 
1.803563   827.7686 
2   6 
1.803561   827.7695 
2   6 
1.801742   1085.968 
2   6 
1.801744   1085.968 
2   6 
1.801742   1085.969 
2   6 
1.801439   1429.074 
2   6 
1.801441   1429.074 
2   6 
1.801439   1429.075 
2   6 
1.802384   1897.181 
2   6 
1.802386   1897.181 
2   6 
1.802384   1897.183 
2   6 
1.803736   2516.667 
2   6 
1.803738   2516.667 
2   6 
1.803736   2516.669 
2   6 
1.804696   3318.821 
2   6 
1.804698   3318.821 
2   6 
1.804696   3318.825 
2   6 
1.804983   4376.606 
2   6 
1.804985   4376.606 
2   6 
1.804983   4376.61 
2   6 
1.804713   5795.494 
2   6 
1.804715   5795.494 
2   6 
1.804713   5795.5 
2   6 
1.804234   7680.955 
2   6 
1.804236   7680.955 
2   6 
1.804234   7680.963 
2   6 
1.803887   10135.01 
2   6 
1.803889   10135.01 
2   6 
1.803887   10135.02 
2   6 
1.803806   13414.33 
2   6 
1.803808   13414.33 
2   6 
1.803806   13414.34 
2   6 
1.803952   17753.74 
2   6 
1.803954   17753.74 
2   6 
1.803952   17753.76 
2   6 
1.804182   23566.76 
2   6 
1.804184   23566.76 
2   6 
1.804182   23566.78 
2   6 
1.804359   31475.27 
2   6 
1.804361   31475.27 
2   6 
1.804359   31475.3 
2   6 
1.8044   40939.58 
2   6 
1.804402   40939.58 
2   6 
1.8044   40939.62 
2   6 
1.80437   51137.62 
2   6 
1.804372   51137.62 
2   6 
1.80437   51137.67 
2   6 
1.804316   61335.65 
2   6 
1.804318   61335.65 
2   6 
1.804316   61335.72 
2   6 
1.804273   71533.69 
2   6 
1.804275   71533.69 
2   6 
1.804273   71533.76 
2   6 
1.804246   81731.73 
2   6 
1.804248   81731.73 
2   6 
1.804246   81731.81 
Fit Mean:  1.804246  Size:  81731.73  Code:  1 
> 
> cat("Final Fit Mean: ", nb_fit_mu, " Size: ", nb_fit_size, " Code: ", nb_fit$code, " Try Size: ", try_size, "\n")
Final Fit Mean:  1.804246  Size:  81731.73  Code:  1  Try Size:  10 
> 
> ## Fit failed = reset parameters so graphing and output code can recognize this
> if ((nb_fit_mu < 0) || (nb_fit_size < 0) || (nb_fit$code != 1))
+ {
+   nb_fit_mu = 0
+   nb_fit_size = 0
+ }
> 
> 
> ## things can go wrong with fitting and we can still end up with invalid values
> 
> fit_nb = c()
> included_fract = 0
> if (nb_fit_mu > 0)
+ {
+   end_fract = pnbinom(end_i_for_fits, mu = nb_fit_mu, size=nb_fit_size)
+   start_fract = pnbinom(start_i_for_fits, mu = nb_fit_mu, size=nb_fit_size)
+   included_fract = end_fract-start_fract;
+ 
+   if (included_fract >= 0.01) {
+ 
+     ## Adjust so that we are back in full coords before making fit!!
+     if (num_per_bin > 1) 
+     {
+       nb_fit_mu = nb_fit_mu * num_per_bin
+     }
+     fit_nb = dnbinom(0:max(X$coverage), mu = nb_fit_mu, size=nb_fit_size)*inner_total/included_fract;
+   }
+ }
> 
> ## If an insufficient amount of fit was included, then invalidate it
> if (included_fract < 0.01)
+ {
+   nb_fit_mu = 0
+   nb_fit_size = 0
+ }
> 
> f_p <- function(par) {
+ 
+   lambda = par[1];
+ 
+   if (lambda <= 0)
+   {
+     return(0);
+   }
+   
+ 	total <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{	
+     #cat(i, " ", lambda, "\n");
+ 		dist[i] <- dpois(i, lambda=lambda);
+ 		total <- total + dist[i] 
+ 	}
+ 	#print (total)
+ 
+  	l <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{
+ 		l <- l + ((X.for.fits[i]/inner_total)-(dist[i]/total))^2;
+ 	}
+ 	return(l);
+ }
> 
> 
> ## Fit Poisson 
> ## - allow fit to fail and set all params to zero/empty if that is the case
> 
> p_fit = NULL
> try(suppressWarnings(p_fit<-nlm(f_p, c(m), print.level=this.print.level)))
> 
> fit_p = c()
> if (!is.null(p_fit) && (p_fit$estimate[1] > 0))
+ {
+   #print (nb_fit$estimate[1])
+   p_fit_lambda = p_fit$estimate[1];
+   #print(0:max(X$coverage))
+ 
+   end_fract = ppois(end_i_for_fits, lambda = p_fit_lambda)
+   start_fract = ppois(start_i_for_fits, lambda = p_fit_lambda)
+   included_fract = end_fract-start_fract;
+ 
+   ## Adjust so that we are back in full coords before making fit!!
+   if (num_per_bin > 1) 
+   {
+     p_fit_lambda = p_fit_lambda * num_per_bin
+   }
+   fit_p<-dpois(0:max(X$coverage), lambda = p_fit_lambda)*inner_total/included_fract;
+ }
> 
> 
> ## Graphing
> ##
> ## don't graph very high values with very little coverage
> i<-max_i
> while (i <= length(X$n) && X$n[i]>0.01*max_n)
+ {		
+ 	i <- i+1;
+ }
> graph_end_i <-i
> 
> ## Ths leaves enough room to the right of the peak for the legend
> graph_end_i = max(floor(2.2 * max_i), graph_end_i);
> 
> ## graphics settings
> my_pch = 21
> my_col = "black";
> my_col_censored = "red";
> 
> if (pdf_output == 0) {
+   
+   ## bitmap() requires ghostscript to be installed.
+   ## taa=4, gaa=2 options NOT compatible with earlier R versions!
+   ## units = "px" NOT compatible with even earlier R versions!
+   
+   if(!capabilities(what = "png"))
+   {
+     ## fallback to ghostscript
+     bitmap(plot_file, height=6, width=7, type = "png16m", res = 72, pointsize=18)
+   } else {
+     ## use X11 function, which gives better resolution
+     png(plot_file, height=6, width=7, units ="in", res = 72, pointsize=18)
+     par(family="sans")
+   }
+ } else {
+   pdf(plot_file, height=6, width=7)
+   par(family="sans")
+ }
> 
> par(mar=c(5.5,7.5,3,1.5));
> 
> max_y = 0
> if (plot_poisson) {
+ 	max_y = max(X$n, fit_p, fit_nb)
+ } else {
+ 	max_y = max(X$n, fit_nb)
+ }
> 
> plot(0:10, 0:10, type="n", lty="solid", ylim=c(0, max_y)*1.05, xlim=c(0, graph_end_i), lwd=1, xaxs="i", yaxs="i", axes=F, las=1, main="Coverage Distribution at Unique-Only Positions", xlab="Coverage depth (reads)", ylab="", cex.lab=1.2, cex.axis=1.2)
> 
> mtext(side = 2, text = "Number of reference positions", line = 5.5, cex=1.2)
> 
> sciNotation <- function(x, digits = 1) {
+     if (length(x) > 1) {
+         return(append(sciNotation(x[1]), sciNotation(x[-1])))     
+ 	} 
+     if (!x) return(0) 
+ 
+ 	exponent <- floor(log10(x)) 
+     base <- round(x / 10^exponent, digits)     
+ 	as.expression(substitute(base %*% 10^exponent, list(base = base, exponent = exponent))) 
+ }
> 
> #axis(2, cex.lab=1.2, las=1, cex.axis=1.2, labels=T, at=(0:6)*50000)
> axis(2, cex.lab=1.2, las=1, cex.axis=1.2, at = axTicks(2), labels = sciNotation(axTicks(2), 1))
> axis(1, cex.lab=1.2, cex.axis=1.2, labels=T)
> box()
> 
> #graph the coverage as points
> fit_data <- subset(X, (coverage>=start_i) & (coverage<=end_i) );
> points(fit_data$coverage, fit_data$n, pch=my_pch, col=my_col, bg="white", cex=1.2)
> 
> #graph the censored coverage as red points
> cat(start_i, " ", end_i, "\n", sep="")
2 6
> 
> censored_data <- subset(X, (coverage<start_i) | (coverage>end_i) );
> points(censored_data$coverage, censored_data$n, pch=my_pch, col=my_col_censored, bg="white", cex=1.2)
> 
> #graph the poisson fit IF REQUESTED
> if (plot_poisson) {
+ 	lines(0:max(X$coverage), fit_p, lwd=3, lty="22", col="black");
+ }
> 
> #graph the negative binomial fit
> if (nb_fit_mu > 0) {
+   lines(0:max(X$coverage), fit_nb, lwd=3, col="black");
+ }
> 
> if (plot_poisson) {
+ 	legend("topright", c("Coverage distribution", "Censored data", "Negative binomial", "Poisson"), lty=c("blank","blank","solid","22"), lwd=c(1,1,2,2), pch=c(my_pch, my_pch, -1, -1), col=c("black", "red", "black", "black"), bty="n")
+ } else {
+ 	legend("topright", c("Coverage distribution", "Censored data", "Negative binomial"), lty=c("blank","blank","solid"), lwd=c(1,1,2), pch=c(my_pch, my_pch, -1), col=c("black", "red", "black"), bty="n")
+ }
> 
> dev.off()
null device 
          1 
> 
> ## Fit the marginal value that we use for propagating deletions
> 
> if (nb_fit_mu > 0) {
+   cat(nb_fit_size, " ", nb_fit_mu, "\n")
+   deletion_propagation_coverage = suppressWarnings(qnbinom(deletion_propagation_pr_cutoff, size = nb_fit_size, mu = nb_fit_mu))
+ } else {
+   cat("Fallback to calculating off an estimate of just variance = mu + mu^2/size\n")
+   size_estimate = (1/(v-m))*(m*m)
+   cat("Mu estimate=", m," Size estimate =", size_estimate, "\n")
+   deletion_propagation_coverage = suppressWarnings(qnbinom(deletion_propagation_pr_cutoff, size = size_estimate, mu = m))
+   if (is.na(deletion_propagation_coverage) || is.nan(deletion_propagation_coverage) || (deletion_propagation_coverage < 1)) {
+     cat("Double fallback to calculating as just 10% of the mean\n")
+     deletion_propagation_coverage = m * 0.1
+   }
+ }
81731.73   1.804246 
> 
> #Don't allow one read to indicate non-deleted regions
> if (deletion_propagation_coverage < 1) {
+     deletion_propagation_coverage = 1
+ }
> 
> #This works fine with the negative values
> #If we have both low fit coverage and low straight average coverage then we're deleted...
> if ( (nb_fit_mu <= 3) && (m <= 3) ) {
+   deletion_propagation_coverage = -1
+ }
> 
> #print out statistics
> 
> print(nb_fit_size);
[1] 81731.73
> print(nb_fit_mu);
[1] 1.804246
> 
> print(m)
[1] 18.57143
> print(v)
[1] 194.9583
> print(D)
[1] 10.49776
> 
> print(deletion_propagation_coverage)
[1] 1
> 
> warnings()
> 
